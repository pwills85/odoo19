# üß† ESTRATEGIA: Entrenamiento de IA con 7 A√±os de Hist√≥rico
## Knowledge Base Empresarial para Clasificaci√≥n Inteligente

**Fecha:** 2025-10-22
**Versi√≥n:** 1.0
**Prioridad:** üü° **ALTA** (Pre-implementaci√≥n - Semana 0)

---

## üéØ OPORTUNIDAD CR√çTICA

### Tienes un Tesoro: 7 A√±os de Datos Reales

**Qu√© significa:**
- Miles de facturas procesadas
- Relaciones Proveedor ‚Üí Producto ‚Üí Cuenta Contable ‚Üí Anal√≠tica
- Patrones de compra establecidos
- Decisiones humanas validadas (ground truth)

**Valor:**
- üöÄ **Accuracy >95%** en clasificaci√≥n (vs 70-80% sin training)
- üöÄ **Reducci√≥n 90%** en revisi√≥n manual
- üöÄ **Aprendizaje de patrones** espec√≠ficos de TU empresa
- üöÄ **Mejora continua** autom√°tica

---

## üìä DATOS DISPONIBLES (7 A√ëOS)

### Tabla 1: Facturas de Proveedores (`account.move`)

```sql
SELECT
    id,
    partner_id,           -- Proveedor
    invoice_date,
    amount_total,
    state,                -- posted = validado
    purchase_id,          -- PO vinculado
    ref                   -- Referencia/N√∫mero factura
FROM account_move
WHERE move_type = 'in_invoice'
  AND state = 'posted'
  AND invoice_date >= '2018-01-01'
ORDER BY invoice_date DESC;
```

**Estimado:** ~10,000 - 50,000 facturas (depende tama√±o empresa)

---

### Tabla 2: L√≠neas de Factura (`account.move.line`)

```sql
SELECT
    aml.id,
    aml.move_id,
    aml.product_id,           -- Producto
    aml.name,                 -- Descripci√≥n
    aml.quantity,
    aml.price_unit,
    aml.account_id,           -- Cuenta contable ‚≠ê
    aml.analytic_distribution,-- Distribuci√≥n anal√≠tica ‚≠ê
    pp.default_code,          -- SKU producto
    pp.categ_id,              -- Categor√≠a
    pc.name AS category_name
FROM account_move_line aml
JOIN account_move am ON am.id = aml.move_id
LEFT JOIN product_product pp ON pp.id = aml.product_id
LEFT JOIN product_category pc ON pc.id = pp.categ_id
WHERE am.move_type = 'in_invoice'
  AND am.state = 'posted'
  AND am.invoice_date >= '2018-01-01'
  AND aml.display_type IS NULL;  -- Excluir l√≠neas de secci√≥n/nota
```

**Estimado:** ~50,000 - 500,000 l√≠neas

---

### Tabla 3: √ìrdenes de Compra (`purchase.order` + `purchase.order.line`)

```sql
SELECT
    po.id,
    po.name,
    po.partner_id,
    po.state,
    pol.id AS line_id,
    pol.product_id,
    pol.name AS description,
    pol.account_analytic_id,  -- ‚≠ê Anal√≠tica en PO
    aa.name AS analytic_name,
    aa.code AS analytic_code
FROM purchase_order po
JOIN purchase_order_line pol ON pol.order_id = po.id
LEFT JOIN account_analytic_account aa ON aa.id = pol.account_analytic_id
WHERE po.state IN ('purchase', 'done')
  AND po.date_order >= '2018-01-01';
```

---

### Tabla 4: Relaciones Proveedor ‚Üí Producto

```sql
SELECT
    rp.id AS partner_id,
    rp.name AS supplier_name,
    rp.vat AS supplier_rut,
    pp.id AS product_id,
    pp.default_code AS product_code,
    pt.name AS product_name,
    pc.name AS category,
    COUNT(DISTINCT aml.move_id) AS num_invoices,
    COUNT(aml.id) AS num_lines,
    SUM(aml.price_subtotal) AS total_purchased,
    AVG(aml.price_unit) AS avg_price
FROM account_move_line aml
JOIN account_move am ON am.id = aml.move_id
JOIN res_partner rp ON rp.id = am.partner_id
JOIN product_product pp ON pp.id = aml.product_id
JOIN product_template pt ON pt.id = pp.product_tmpl_id
LEFT JOIN product_category pc ON pc.id = pt.categ_id
WHERE am.move_type = 'in_invoice'
  AND am.state = 'posted'
  AND am.invoice_date >= '2018-01-01'
GROUP BY rp.id, rp.name, rp.vat, pp.id, pp.default_code, pt.name, pc.name
HAVING COUNT(aml.id) >= 3  -- Al menos 3 compras
ORDER BY total_purchased DESC;
```

---

## üß† ESTRATEGIA DE ENTRENAMIENTO

### Fase 1: Preparaci√≥n de Datos (Semana 0 - D√≠as 1-2)

#### 1.1 Extracci√≥n de Dataset

```python
# ai-service/training/data_extraction.py

import pandas as pd
import psycopg2
from datetime import datetime, timedelta

class HistoricalDataExtractor:
    """
    Extrae 7 a√±os de datos de Odoo PostgreSQL
    """

    def __init__(self, db_config):
        self.conn = psycopg2.connect(**db_config)

    def extract_training_dataset(self, years=7):
        """
        Extrae dataset completo para training
        """

        date_from = datetime.now() - timedelta(days=years*365)

        # Query principal: l√≠neas de factura con toda la metadata
        query = """
        SELECT
            -- Invoice metadata
            am.id AS invoice_id,
            am.name AS invoice_number,
            am.invoice_date,
            am.partner_id,
            rp.name AS supplier_name,
            rp.vat AS supplier_rut,

            -- Line data
            aml.id AS line_id,
            aml.name AS line_description,
            aml.quantity,
            aml.price_unit,
            aml.price_subtotal,

            -- Product data
            aml.product_id,
            pp.default_code AS product_code,
            pt.name AS product_name,
            pc.name AS product_category,

            -- Accounting (GROUND TRUTH) ‚≠ê
            aml.account_id AS account_id,
            aa.code AS account_code,
            aa.name AS account_name,

            -- Analytics (GROUND TRUTH) ‚≠ê
            aml.analytic_distribution,

            -- PO link
            am.purchase_id,
            po.name AS po_number

        FROM account_move_line aml
        JOIN account_move am ON am.id = aml.move_id
        JOIN res_partner rp ON rp.id = am.partner_id
        LEFT JOIN product_product pp ON pp.id = aml.product_id
        LEFT JOIN product_template pt ON pt.id = pp.product_tmpl_id
        LEFT JOIN product_category pc ON pc.id = pt.categ_id
        LEFT JOIN account_account aa ON aa.id = aml.account_id
        LEFT JOIN purchase_order po ON po.id = am.purchase_id

        WHERE am.move_type = 'in_invoice'
          AND am.state = 'posted'
          AND am.invoice_date >= %(date_from)s
          AND aml.display_type IS NULL

        ORDER BY am.invoice_date DESC;
        """

        df = pd.read_sql(query, self.conn, params={'date_from': date_from})

        print(f"‚úÖ Extracted {len(df):,} invoice lines from {years} years")

        return df


    def extract_supplier_product_patterns(self):
        """
        Patrones: ¬øQu√© compra cada proveedor normalmente?
        """

        query = """
        SELECT
            rp.id AS supplier_id,
            rp.name AS supplier_name,
            rp.vat AS supplier_rut,
            pp.id AS product_id,
            pt.name AS product_name,
            pc.name AS category,
            COUNT(DISTINCT am.id) AS times_purchased,
            COUNT(aml.id) AS total_lines,
            AVG(aml.quantity) AS avg_quantity,
            AVG(aml.price_unit) AS avg_price,
            STDDEV(aml.price_unit) AS price_stddev,

            -- Most common account
            MODE() WITHIN GROUP (ORDER BY aml.account_id) AS most_common_account_id,

            -- Most common analytic
            MODE() WITHIN GROUP (ORDER BY aml.analytic_distribution::text) AS most_common_analytic

        FROM account_move_line aml
        JOIN account_move am ON am.id = aml.move_id
        JOIN res_partner rp ON rp.id = am.partner_id
        LEFT JOIN product_product pp ON pp.id = aml.product_id
        LEFT JOIN product_template pt ON pt.id = pp.product_tmpl_id
        LEFT JOIN product_category pc ON pc.id = pt.categ_id

        WHERE am.move_type = 'in_invoice'
          AND am.state = 'posted'
          AND am.invoice_date >= NOW() - INTERVAL '7 years'

        GROUP BY rp.id, rp.name, rp.vat, pp.id, pt.name, pc.name
        HAVING COUNT(aml.id) >= 3

        ORDER BY times_purchased DESC;
        """

        df = pd.read_sql(query, self.conn)

        print(f"‚úÖ Extracted {len(df):,} supplier-product patterns")

        return df


    def extract_account_analytic_rules(self):
        """
        Reglas: ¬øQu√© cuenta contable + anal√≠tica se usa para qu√©?
        """

        query = """
        SELECT
            pc.name AS product_category,
            aa.code AS account_code,
            aa.name AS account_name,
            aml.analytic_distribution,
            COUNT(*) AS frequency,
            AVG(aml.price_subtotal) AS avg_amount,

            -- Example suppliers
            STRING_AGG(DISTINCT rp.name, ', ') AS example_suppliers,

            -- Example products
            STRING_AGG(DISTINCT pt.name, ', ') AS example_products

        FROM account_move_line aml
        JOIN account_move am ON am.id = aml.move_id
        JOIN res_partner rp ON rp.id = am.partner_id
        JOIN account_account aa ON aa.id = aml.account_id
        LEFT JOIN product_product pp ON pp.id = aml.product_id
        LEFT JOIN product_template pt ON pt.id = pp.product_tmpl_id
        LEFT JOIN product_category pc ON pc.id = pt.categ_id

        WHERE am.move_type = 'in_invoice'
          AND am.state = 'posted'
          AND am.invoice_date >= NOW() - INTERVAL '7 years'

        GROUP BY pc.name, aa.code, aa.name, aml.analytic_distribution
        HAVING COUNT(*) >= 10  -- Al menos 10 usos

        ORDER BY frequency DESC;
        """

        df = pd.read_sql(query, self.conn)

        print(f"‚úÖ Extracted {len(df):,} account-analytic rules")

        return df


# Ejecutar extracci√≥n
if __name__ == '__main__':
    extractor = HistoricalDataExtractor({
        'host': 'localhost',
        'port': 5432,
        'database': 'odoo',
        'user': 'odoo',
        'password': 'odoo'
    })

    # 1. Dataset completo
    df_lines = extractor.extract_training_dataset(years=7)
    df_lines.to_parquet('data/historical_invoice_lines.parquet')

    # 2. Patrones proveedor-producto
    df_patterns = extractor.extract_supplier_product_patterns()
    df_patterns.to_parquet('data/supplier_product_patterns.parquet')

    # 3. Reglas cuenta-anal√≠tica
    df_rules = extractor.extract_account_analytic_rules()
    df_rules.to_parquet('data/account_analytic_rules.parquet')

    print("‚úÖ All data extracted successfully!")
```

---

### Fase 2: Creaci√≥n de Embeddings (Semana 0 - D√≠a 3)

#### 2.1 Embeddings de Productos

```python
# ai-service/training/create_embeddings.py

from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import faiss

class EmbeddingCreator:
    """
    Crea embeddings de todo el hist√≥rico
    """

    def __init__(self):
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    def create_product_embeddings(self, df_lines):
        """
        Embedding por producto basado en descripciones hist√≥ricas
        """

        # Agrupar por producto
        product_texts = df_lines.groupby('product_id').agg({
            'product_name': 'first',
            'product_category': 'first',
            'line_description': lambda x: ' | '.join(x.unique()[:10])  # Top 10 descripciones
        }).reset_index()

        # Crear texto rico para embedding
        product_texts['full_text'] = (
            product_texts['product_category'] + ' - ' +
            product_texts['product_name'] + ' - ' +
            product_texts['line_description']
        )

        # Generate embeddings
        embeddings = self.model.encode(
            product_texts['full_text'].tolist(),
            show_progress_bar=True
        )

        # Create FAISS index for fast search
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)
        faiss.normalize_L2(embeddings)
        index.add(embeddings)

        # Save
        np.save('data/product_embeddings.npy', embeddings)
        faiss.write_index(index, 'data/product_embeddings.faiss')
        product_texts.to_parquet('data/product_metadata.parquet')

        print(f"‚úÖ Created embeddings for {len(product_texts):,} products")

        return embeddings, index, product_texts


    def create_supplier_embeddings(self, df_patterns):
        """
        Embedding por proveedor basado en qu√© vende
        """

        # Agrupar por proveedor
        supplier_texts = df_patterns.groupby('supplier_id').agg({
            'supplier_name': 'first',
            'supplier_rut': 'first',
            'product_name': lambda x: ' | '.join(x.unique()[:20]),  # Top 20 productos
            'category': lambda x: ', '.join(x.unique())
        }).reset_index()

        supplier_texts['full_text'] = (
            supplier_texts['supplier_name'] + ' - ' +
            'Productos: ' + supplier_texts['product_name'] + ' - ' +
            'Categor√≠as: ' + supplier_texts['category']
        )

        embeddings = self.model.encode(
            supplier_texts['full_text'].tolist(),
            show_progress_bar=True
        )

        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        index.add(embeddings)

        np.save('data/supplier_embeddings.npy', embeddings)
        faiss.write_index(index, 'data/supplier_embeddings.faiss')
        supplier_texts.to_parquet('data/supplier_metadata.parquet')

        print(f"‚úÖ Created embeddings for {len(supplier_texts):,} suppliers")

        return embeddings, index, supplier_texts


# Ejecutar
if __name__ == '__main__':
    df_lines = pd.read_parquet('data/historical_invoice_lines.parquet')
    df_patterns = pd.read_parquet('data/supplier_product_patterns.parquet')

    creator = EmbeddingCreator()
    creator.create_product_embeddings(df_lines)
    creator.create_supplier_embeddings(df_patterns)
```

---

### Fase 3: Modelo de Clasificaci√≥n (Semana 0 - D√≠a 4)

#### 3.1 Classifier para Cuenta Contable

```python
# ai-service/training/train_classifier.py

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import joblib

class AccountClassifier:
    """
    Modelo ML para predecir cuenta contable
    """

    def __init__(self):
        self.model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        self.le_account = LabelEncoder()
        self.le_category = LabelEncoder()
        self.le_supplier = LabelEncoder()

    def prepare_features(self, df):
        """
        Feature engineering
        """

        features = pd.DataFrame()

        # Categorical features (encoded)
        features['product_category'] = self.le_category.fit_transform(
            df['product_category'].fillna('Unknown')
        )
        features['supplier_id'] = self.le_supplier.fit_transform(
            df['supplier_rut'].fillna('Unknown')
        )

        # Numerical features
        features['quantity'] = df['quantity']
        features['price_unit'] = df['price_unit']
        features['price_subtotal'] = df['price_subtotal']
        features['log_price'] = np.log1p(df['price_unit'])

        # Temporal features
        df['invoice_date'] = pd.to_datetime(df['invoice_date'])
        features['month'] = df['invoice_date'].dt.month
        features['quarter'] = df['invoice_date'].dt.quarter
        features['year'] = df['invoice_date'].dt.year

        # Has PO?
        features['has_po'] = (~df['purchase_id'].isna()).astype(int)

        # Target
        target = self.le_account.fit_transform(df['account_code'])

        return features, target

    def train(self, df_lines):
        """
        Entrenar modelo
        """

        print("Preparing features...")
        X, y = self.prepare_features(df_lines)

        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"Training set: {len(X_train):,} samples")
        print(f"Test set: {len(X_test):,} samples")

        # Train
        print("Training model...")
        self.model.fit(X_train, y_train)

        # Evaluate
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)

        print(f"‚úÖ Train accuracy: {train_score:.2%}")
        print(f"‚úÖ Test accuracy: {test_score:.2%}")

        # Cross-validation
        cv_scores = cross_val_score(self.model, X, y, cv=5)
        print(f"‚úÖ Cross-validation: {cv_scores.mean():.2%} (+/- {cv_scores.std():.2%})")

        # Feature importance
        importance = pd.DataFrame({
            'feature': X.columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\nTop 5 features:")
        print(importance.head())

        return train_score, test_score

    def predict_account(self, supplier_rut, category, quantity, price, date=None):
        """
        Predecir cuenta contable para nueva compra
        """

        # Prepare input
        input_data = pd.DataFrame([{
            'product_category': category,
            'supplier_rut': supplier_rut,
            'quantity': quantity,
            'price_unit': price,
            'price_subtotal': quantity * price,
            'log_price': np.log1p(price),
            'month': date.month if date else 1,
            'quarter': (date.month - 1) // 3 + 1 if date else 1,
            'year': date.year if date else 2025,
            'has_po': 1,
            'purchase_id': None
        }])

        features, _ = self.prepare_features(input_data)

        # Predict
        prediction = self.model.predict(features)[0]
        probabilities = self.model.predict_proba(features)[0]

        # Decode
        account_code = self.le_account.inverse_transform([prediction])[0]
        confidence = probabilities.max()

        return {
            'account_code': account_code,
            'confidence': float(confidence),
            'top_3': [
                {
                    'account_code': self.le_account.inverse_transform([i])[0],
                    'probability': float(p)
                }
                for i, p in sorted(
                    enumerate(probabilities),
                    key=lambda x: x[1],
                    reverse=True
                )[:3]
            ]
        }

    def save(self, path='models/account_classifier.joblib'):
        """Save model"""
        joblib.dump({
            'model': self.model,
            'le_account': self.le_account,
            'le_category': self.le_category,
            'le_supplier': self.le_supplier
        }, path)
        print(f"‚úÖ Model saved to {path}")

    @classmethod
    def load(cls, path='models/account_classifier.joblib'):
        """Load model"""
        data = joblib.load(path)
        classifier = cls()
        classifier.model = data['model']
        classifier.le_account = data['le_account']
        classifier.le_category = data['le_category']
        classifier.le_supplier = data['le_supplier']
        return classifier


# Entrenar
if __name__ == '__main__':
    df_lines = pd.read_parquet('data/historical_invoice_lines.parquet')

    classifier = AccountClassifier()
    classifier.train(df_lines)
    classifier.save()

    # Test
    result = classifier.predict_account(
        supplier_rut='12345678-9',
        category='Computadores',
        quantity=10,
        price=500000
    )
    print(f"\nTest prediction: {result}")
```

---

### Fase 4: Claude Fine-Tuning con Prompt Engineering (Semana 0 - D√≠a 5)

#### 4.1 Crear Knowledge Base para Claude

```python
# ai-service/training/create_claude_knowledge_base.py

import json
import pandas as pd

class ClaudeKnowledgeBaseBuilder:
    """
    Construye knowledge base para Claude con ejemplos hist√≥ricos
    """

    def build_supplier_product_kb(self, df_patterns):
        """
        KB: ¬øQu√© vende cada proveedor normalmente?
        """

        kb = {}

        for supplier_rut, group in df_patterns.groupby('supplier_rut'):
            kb[supplier_rut] = {
                'supplier_name': group.iloc[0]['supplier_name'],
                'typical_products': group.nlargest(10, 'times_purchased')[[
                    'product_name', 'category', 'times_purchased',
                    'most_common_account_id', 'most_common_analytic'
                ]].to_dict('records')
            }

        with open('data/kb_supplier_products.json', 'w') as f:
            json.dump(kb, f, indent=2)

        print(f"‚úÖ Created supplier-product KB for {len(kb):,} suppliers")

        return kb


    def build_account_rules_kb(self, df_rules):
        """
        KB: ¬øQu√© cuenta contable para qu√© categor√≠a?
        """

        kb = []

        for _, row in df_rules.head(100).iterrows():  # Top 100 reglas
            kb.append({
                'category': row['product_category'],
                'account_code': row['account_code'],
                'account_name': row['account_name'],
                'analytic_distribution': row['analytic_distribution'],
                'frequency': int(row['frequency']),
                'avg_amount': float(row['avg_amount']),
                'example_suppliers': row['example_suppliers'],
                'example_products': row['example_products']
            })

        with open('data/kb_account_rules.json', 'w') as f:
            json.dump(kb, f, indent=2)

        print(f"‚úÖ Created account-rules KB with {len(kb)} rules")

        return kb


    def build_analytic_patterns_kb(self, df_lines):
        """
        KB: Patrones de cuentas anal√≠ticas
        """

        # Extraer patrones
        patterns = df_lines.groupby([
            'product_category',
            'supplier_name'
        ]).agg({
            'analytic_distribution': lambda x: x.mode()[0] if len(x.mode()) > 0 else None,
            'line_id': 'count'
        }).rename(columns={'line_id': 'frequency'}).reset_index()

        patterns = patterns[patterns['frequency'] >= 5]  # M√≠nimo 5 usos
        patterns = patterns.nlargest(200, 'frequency')  # Top 200

        kb = patterns.to_dict('records')

        with open('data/kb_analytic_patterns.json', 'w') as f:
            json.dump(kb, f, indent=2)

        print(f"‚úÖ Created analytic-patterns KB with {len(kb)} patterns")

        return kb


    def create_claude_system_prompt(self, kb_supplier, kb_rules, kb_analytic):
        """
        Crear system prompt enriquecido con conocimiento hist√≥rico
        """

        prompt = f"""Eres un experto en contabilidad de una empresa chilena con 7 a√±os de experiencia.

TU CONOCIMIENTO HIST√ìRICO:

1. PROVEEDORES Y SUS PRODUCTOS T√çPICOS:
{json.dumps(list(kb_supplier.values())[:10], indent=2, ensure_ascii=False)}
... (base completa disponible)

2. REGLAS DE CLASIFICACI√ìN CONTABLE (Top 20):
{json.dumps(kb_rules[:20], indent=2, ensure_ascii=False)}
... (base completa disponible)

3. PATRONES DE CUENTAS ANAL√çTICAS (Top 20):
{json.dumps(kb_analytic[:20], indent=2, ensure_ascii=False)}
... (base completa disponible)

INSTRUCCIONES:
- Cuando recibas una factura nueva, usa este conocimiento hist√≥rico para clasificarla
- Si el proveedor es conocido, usa sus patrones t√≠picos
- Si la categor√≠a de producto es conocida, usa las reglas establecidas
- Mant√©n consistencia con decisiones hist√≥ricas
- Si hay ambig√ºedad, pregunta o marca para revisi√≥n manual

Responde siempre en JSON con:
{{
  "account_code": "...",
  "account_name": "...",
  "analytic_account_id": ID,
  "analytic_distribution": {{"ID": 100.0}},
  "confidence": 0-1,
  "reasoning": "explicaci√≥n basada en hist√≥rico"
}}
"""

        with open('data/claude_system_prompt.txt', 'w') as f:
            f.write(prompt)

        print(f"‚úÖ Created Claude system prompt ({len(prompt)} chars)")

        return prompt


# Ejecutar
if __name__ == '__main__':
    df_patterns = pd.read_parquet('data/supplier_product_patterns.parquet')
    df_rules = pd.read_parquet('data/account_analytic_rules.parquet')
    df_lines = pd.read_parquet('data/historical_invoice_lines.parquet')

    builder = ClaudeKnowledgeBaseBuilder()

    kb_supplier = builder.build_supplier_product_kb(df_patterns)
    kb_rules = builder.build_account_rules_kb(df_rules)
    kb_analytic = builder.build_analytic_patterns_kb(df_lines)

    prompt = builder.create_claude_system_prompt(kb_supplier, kb_rules, kb_analytic)
```

---

## üöÄ INTEGRACI√ìN EN AI SERVICE

### Actualizar AI Service con Modelos Entrenados

```python
# ai-service/main.py

from training.train_classifier import AccountClassifier
import json
import faiss
import numpy as np

# Load modelos en startup
account_classifier = AccountClassifier.load('models/account_classifier.joblib')

# Load embeddings
product_embeddings_index = faiss.read_index('data/product_embeddings.faiss')
supplier_embeddings_index = faiss.read_index('data/supplier_embeddings.faiss')

# Load knowledge bases
with open('data/kb_supplier_products.json') as f:
    kb_suppliers = json.load(f)

with open('data/kb_account_rules.json') as f:
    kb_rules = json.load(f)

with open('data/claude_system_prompt.txt') as f:
    claude_system_prompt = f.read()


@app.post("/api/ai/reception/classify_with_history")
async def classify_invoice_line_with_historical_knowledge(
    request: InvoiceLineClassificationRequest
):
    """
    Clasificar l√≠nea de factura usando conocimiento hist√≥rico

    Combina:
    1. ML classifier (7 a√±os training)
    2. Embeddings similarity
    3. Claude con knowledge base
    """

    # 1. ML Classifier prediction
    ml_prediction = account_classifier.predict_account(
        supplier_rut=request.supplier_rut,
        category=request.product_category,
        quantity=request.quantity,
        price=request.price_unit
    )

    # 2. Similar products (embeddings)
    query_text = f"{request.product_category} - {request.product_name} - {request.description}"
    query_embedding = embedding_model.encode([query_text])
    faiss.normalize_L2(query_embedding)

    distances, indices = product_embeddings_index.search(query_embedding, k=5)
    similar_products = [
        {
            'product': product_metadata.iloc[idx]['product_name'],
            'similarity': float(1 - distances[0][i])
        }
        for i, idx in enumerate(indices[0])
    ]

    # 3. Historical patterns for supplier
    supplier_history = kb_suppliers.get(request.supplier_rut, {})

    # 4. Claude decision with full context
    claude_prompt = f"""
Clasifica esta l√≠nea de factura:

DATOS DE LA L√çNEA:
- Proveedor: {request.supplier_name} (RUT: {request.supplier_rut})
- Producto: {request.product_name}
- Categor√≠a: {request.product_category}
- Descripci√≥n: {request.description}
- Cantidad: {request.quantity}
- Precio unitario: ${request.price_unit:,.0f}

CONTEXTO ML (7 a√±os de datos):
- Predicci√≥n cuenta: {ml_prediction['account_code']} (confianza: {ml_prediction['confidence']:.0%})
- Top 3 cuentas sugeridas: {json.dumps(ml_prediction['top_3'], indent=2)}

PRODUCTOS SIMILARES HIST√ìRICOS:
{json.dumps(similar_products, indent=2)}

HISTORIAL DE ESTE PROVEEDOR:
{json.dumps(supplier_history, indent=2)}

Bas√°ndote en TODO el contexto hist√≥rico, ¬øcu√°l es la clasificaci√≥n correcta?
Responde en JSON como se especific√≥ en el system prompt.
"""

    response = anthropic.messages.create(
        model="claude-3-5-sonnet-20241022",
        system=claude_system_prompt,
        messages=[{"role": "user", "content": claude_prompt}],
        max_tokens=2000
    )

    claude_decision = json.loads(response.content[0].text)

    # 5. Combine predictions (ensemble)
    final_decision = {
        'ml_prediction': ml_prediction,
        'claude_decision': claude_decision,
        'similar_products': similar_products,
        'supplier_history_available': len(supplier_history) > 0,

        # Final recommendation (prefer Claude if high confidence)
        'recommended_account': (
            claude_decision['account_code']
            if claude_decision['confidence'] > 0.85
            else ml_prediction['account_code']
        ),
        'recommended_analytic': claude_decision.get('analytic_distribution'),
        'overall_confidence': max(
            ml_prediction['confidence'],
            claude_decision['confidence']
        ),
        'reasoning': claude_decision['reasoning']
    }

    return final_decision
```

---

## üìä APRENDIZAJE CONTINUO

### Reentrenar con Nuevos Datos

```python
# ai-service/training/continuous_learning.py

class ContinuousLearner:
    """
    Re-entrenar modelos peri√≥dicamente con datos nuevos
    """

    def should_retrain(self):
        """
        Verificar si es momento de re-entrenar
        """

        # Criteria:
        # - Han pasado 3 meses desde √∫ltimo training
        # - O hay 1000+ nuevas facturas validadas
        # O accuracy ha bajado < 85%

        last_train_date = self.get_last_train_date()
        new_invoices_count = self.count_new_invoices(last_train_date)
        current_accuracy = self.calculate_recent_accuracy()

        should_retrain = (
            (datetime.now() - last_train_date).days >= 90 or
            new_invoices_count >= 1000 or
            current_accuracy < 0.85
        )

        return should_retrain

    def retrain_all_models(self):
        """
        Re-entrenar todos los modelos con datos actualizados
        """

        print("üîÑ Starting retraining process...")

        # 1. Extract new data (7 a√±os + nuevos datos)
        extractor = HistoricalDataExtractor(db_config)
        df_lines = extractor.extract_training_dataset(years=7)

        # 2. Retrain classifier
        classifier = AccountClassifier()
        train_score, test_score = classifier.train(df_lines)

        if test_score >= 0.85:  # Quality gate
            classifier.save(f'models/account_classifier_v{version}.joblib')
            print(f"‚úÖ Classifier retrained: {test_score:.2%} accuracy")
        else:
            print(f"‚ö†Ô∏è Retraining failed: only {test_score:.2%} accuracy")
            return False

        # 3. Recreate embeddings
        creator = EmbeddingCreator()
        creator.create_product_embeddings(df_lines)

        # 4. Update Claude knowledge base
        builder = ClaudeKnowledgeBaseBuilder()
        # ... rebuild KB

        # 5. Deploy new models (atomic swap)
        self.deploy_new_models(version)

        print(f"‚úÖ Retraining complete! Version {version} deployed.")

        return True


# Cron job (monthly)
@app.post("/api/ai/admin/retrain")
async def trigger_retraining(admin_key: str = Header(...)):
    """
    Endpoint para re-entrenar modelos (admin only)
    """

    if admin_key != os.getenv('ADMIN_API_KEY'):
        raise HTTPException(403, "Unauthorized")

    learner = ContinuousLearner()

    if learner.should_retrain():
        success = learner.retrain_all_models()
        return {"retrained": success}
    else:
        return {"retrained": False, "reason": "Not needed yet"}
```

---

## üìà M√âTRICAS ESPERADAS CON TRAINING

| M√©trica | Sin Training | Con 7 A√±os Training | Mejora |
|---------|-------------|---------------------|--------|
| **Account accuracy** | 70-80% | **95-98%** | +20% |
| **Analytic accuracy** | 75-85% | **92-96%** | +15% |
| **Revisi√≥n manual** | 30-40% | **5-10%** | -75% |
| **Tiempo clasificaci√≥n** | 3-5 min | **<30 seg** | -90% |
| **Errores** | 10-15% | **<2%** | -85% |

---

## ‚è±Ô∏è TIMELINE DE IMPLEMENTACI√ìN

### Semana 0 (Pre-implementaci√≥n): Training (5 d√≠as)

**D√≠a 1-2:** Extracci√≥n de datos
- [ ] Query hist√≥rico (7 a√±os)
- [ ] Limpiar y validar datos
- [ ] Export a Parquet

**D√≠a 3:** Embeddings
- [ ] Product embeddings
- [ ] Supplier embeddings
- [ ] FAISS indexes

**D√≠a 4:** ML Models
- [ ] Train account classifier
- [ ] Train analytic classifier
- [ ] Validar accuracy >90%

**D√≠a 5:** Claude KB
- [ ] Build knowledge bases
- [ ] Create enriched system prompt
- [ ] Test con ejemplos reales

---

## ‚úÖ CHECKLIST DE PREPARACI√ìN

### Datos
- [ ] Acceso a PostgreSQL de Odoo
- [ ] 7 a√±os de facturas validadas (`state='posted'`)
- [ ] Cuentas contables consistentes
- [ ] Cuentas anal√≠ticas bien asignadas

### Infraestructura
- [ ] AI Service con suficiente RAM (8GB+)
- [ ] Disk space para modelos (5GB+)
- [ ] GPU opcional (acelera training)

### Librer√≠as
- [ ] pandas, numpy
- [ ] scikit-learn
- [ ] sentence-transformers
- [ ] faiss-cpu (o faiss-gpu)
- [ ] anthropic
- [ ] psycopg2

---

## üéØ RESULTADO FINAL

Con 7 a√±os de hist√≥rico entrenado:

‚úÖ **IA aprende patrones espec√≠ficos** de TU empresa
‚úÖ **Conoce proveedores** y qu√© venden normalmente
‚úÖ **Sabe qu√© cuenta contable** usar para cada categor√≠a
‚úÖ **Entiende cuentas anal√≠ticas** por proyecto/centro costo
‚úÖ **Detecta anomal√≠as** comparando con hist√≥rico
‚úÖ **Mejora continuamente** con cada factura nueva
‚úÖ **95%+ accuracy** en clasificaci√≥n

**Tu empresa tendr√° un sistema de IA que:**
- Piensa como tu equipo de contabilidad
- Conoce tus proveedores y productos
- Mantiene consistencia hist√≥rica
- Mejora con el tiempo

---

**Documento creado:** 2025-10-22
**Versi√≥n:** 1.0
**Estado:** ‚úÖ Listo para implementaci√≥n

**¬øQuieres que comencemos con la extracci√≥n del hist√≥rico?** üöÄ
