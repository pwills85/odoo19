# üìö ESTRATEGIA DE PROPAGACI√ìN: Conocimiento sobre Prompting Efectivo

**Fecha**: 2025-11-11  
**Contexto**: Experimento de locuacidad valid√≥ t√©cnicas de prompting que generan 13x variaci√≥n en output  
**Objetivo**: Propagar hallazgos cr√≠ticos a todos los CLIs y documentaci√≥n del proyecto

---

## üéØ HALLAZGOS CLAVE A PROPAGAR

### Descubrimiento Principal
**La "locuacidad" de Claude (y otros LLMs) var√≠a 13x-25x** seg√∫n:
1. **Complejidad del prompt** (P1‚ÜíP4: 100‚Üí1,303 palabras)
2. **System prompt** (CLI conciso vs Web conversacional: 4-6x)
3. **Temperature** (0.05-0.4 t√©cnico vs 0.7 general: 2-3x)
4. **Context window** (KB estructurado vs historial: 1.5-2x)

### M√°ximas de Prompting Validadas

```yaml
Brevedad Extrema (P1 - Consultas Factuales):
  - Prompts: "Lista X", "¬øQu√© hay en Y?"
  - Output esperado: 70-150 palabras
  - Caracter√≠sticas: Listas concisas, 0 an√°lisis
  - Ejemplo: "Lista servicios Docker" ‚Üí 76 palabras

An√°lisis T√©cnico (P2 - Consultas Medias):
  - Prompts: "Analiza archivo X", "Describe modelo Y"
  - Output esperado: 300-400 palabras
  - Caracter√≠sticas: Tabla + secciones, 1 code block
  - Ejemplo: "Analiza account_move" ‚Üí 342 palabras

An√°lisis Profundo (P3 - Multi-M√≥dulo):
  - Prompts: "Compara arquitecturas X, Y, Z"
  - Output esperado: 800-1,000 palabras
  - Caracter√≠sticas: Headers, tablas, file refs, t√©rminos t√©cnicos
  - Ejemplo: "Compara 3 m√≥dulos l10n_cl" ‚Üí 915 palabras

An√°lisis Arquitect√≥nico (P4 - Desarrollo Real):
  - Prompts: "Eval√∫a decisiones de dise√±o en sistema X con N componentes"
  - Output esperado: 1,200-1,500 palabras
  - Caracter√≠sticas: 55 headers, 21 tablas, 31 file refs, 109 t√©rminos t√©cnicos
  - Especificidad: 0.95/1.0 (m√°xima precisi√≥n)
  - Ejemplo: "Analiza arquitectura sistema migraci√≥n" ‚Üí 1,303 palabras
```

---

## üìÅ ESTRUCTURA DE PROPAGACI√ìN

### 1. Directorio Central: `docs/prompts_desarrollo/`

**Agregar archivos nuevos:**

```
docs/prompts_desarrollo/
‚îú‚îÄ‚îÄ README.md (ya existe - actualizar)
‚îú‚îÄ‚îÄ MAXIMAS_DESARROLLO.md (ya existe - actualizar)
‚îú‚îÄ‚îÄ MAXIMAS_AUDITORIA.md (ya existe - revisar)
‚îÇ
‚îú‚îÄ‚îÄ ‚≠ê NUEVO: ESTRATEGIA_PROMPTING_EFECTIVO.md
‚îÇ   ‚îî‚îÄ‚îÄ Gu√≠a completa basada en experimento
‚îÇ
‚îú‚îÄ‚îÄ ‚≠ê NUEVO: EJEMPLOS_PROMPTS_POR_NIVEL.md
‚îÇ   ‚îî‚îÄ‚îÄ P1-P4 con templates y outputs esperados
‚îÇ
‚îî‚îÄ‚îÄ ‚≠ê NUEVO: METRICAS_CALIDAD_RESPUESTAS.md
    ‚îî‚îÄ‚îÄ C√≥mo medir especificidad, densidad t√©cnica, file refs
```

---

## ü§ñ PROPAGACI√ìN A CLIs ESPEC√çFICOS

### A. GitHub Copilot CLI (.github/copilot-instructions.md)

**Status actual**: ‚úÖ YA √ìPTIMO (474 l√≠neas, brevedad configurada)

**Mejoras sugeridas**:

```markdown
## üìä Adaptaci√≥n de Respuestas por Complejidad

### Consultas Simples (P1)
Para preguntas factuales tipo "lista X", "¬øqu√© hay en Y?":
- Target: 70-150 palabras
- Formato: Lista concisa sin an√°lisis
- Ejemplo: "Lista servicios Docker" ‚Üí output ultra-breve

### An√°lisis T√©cnicos (P2-P3)
Para an√°lisis de archivos o comparaciones:
- Target P2: 300-400 palabras (1 archivo)
- Target P3: 800-1,000 palabras (m√∫ltiples archivos)
- Formato: Headers + tablas + code snippets

### An√°lisis Arquitect√≥nicos (P4)
Para evaluaci√≥n de sistemas complejos (2,000+ l√≠neas c√≥digo):
- Target: 1,200-1,500 palabras
- Formato: Estructura profesional con:
  - 30+ file references expl√≠citos
  - 100+ technical terms
  - 30+ code snippets con soluciones
  - 20+ tablas comparativas
  - Especificidad 0.90+ (m√°xima precisi√≥n)

**M√©tricas de calidad esperadas**:
- Especificidad score: 0.50 (P1) ‚Üí 0.95 (P4)
- Densidad t√©cnica: 0 (P1) ‚Üí 8.37 t√©rminos/100 palabras (P4)
- File references: 0 (P1-P2) ‚Üí 31 (P4)
```

### B. Claude Code (.claude/project/)

**Archivos a actualizar:**

1. **`.claude/project/PROMPTING_BEST_PRACTICES.md`** (NUEVO)

```markdown
# Mejores Pr√°cticas de Prompting - Validadas Experimentalmente

## Resultados del Experimento (2025-11-11)

6 prompts ejecutados, escalamiento 13x validado:
- P1 (Simple): 100 palabras, especificidad 0.53
- P2 (Medio): 342 palabras, especificidad 0.56
- P3 (Complejo): 915 palabras, especificidad 0.74
- P4 (Cr√≠tico): 1,303 palabras, especificidad 0.95 ‚≠ê

## Templates por Nivel

### P1 - Consultas Factuales
```
[Verbo acci√≥n simple] + [Objeto concreto]
Ejemplos:
- "Lista servicios Docker activos"
- "Muestra m√≥dulos en addons/localization/"
- "Valida RUT 76876876-8"
```

### P2 - An√°lisis T√©cnico B√°sico
```
Analiza [archivo/componente] y eval√∫a:
1. [Aspecto t√©cnico 1]
2. [Aspecto t√©cnico 2]
3. [Aspecto t√©cnico 3]
```

### P3 - Comparaci√≥n Multi-Componente
```
Compara arquitectura de [m√≥dulo A], [m√≥dulo B], [m√≥dulo C]:
- Patrones de herencia
- Naming conventions
- Uso de libs/ vs service layer
- Identificar inconsistencias
```

### P4 - An√°lisis Arquitect√≥nico Profundo
```
Analiza cr√≠ticamente la arquitectura de [sistema completo]:

Contexto: [Descripci√≥n detallada de N componentes, X l√≠neas totales]

Eval√∫a:
1. Dise√±o de N capas (separaci√≥n, flujo datos, patrones)
2. Estrategia de [t√©cnica espec√≠fica] (herramientas, edge cases)
3. Sistema de seguridad (redundancia, fallos, rollback)
4. Validaci√≥n [tipo] (suficiencia, completitud)
5. Escalabilidad y performance (cuellos de botella)
6. Trade-offs (automatizaci√≥n vs seguridad, velocidad vs exhaustividad)
7. Mejoras potenciales (cambios arquitect√≥nicos, funcionalidad faltante)

Archivos a analizar: [lista espec√≠fica con paths]
Entregable: An√°lisis profesional con decisiones de dise√±o, fortalezas,
debilidades, recomendaciones con c√≥digo, evaluaci√≥n trade-offs
```

## M√©tricas de Calidad

### C√≥mo Medir Especificidad
```python
specificity_score = (
    (file_refs / 10) * 0.30 +
    (tech_terms / 20) * 0.25 +
    (code_blocks / 10) * 0.20 +
    (numbers / 50) * 0.15 +
    (percentages / 10) * 0.10
)
# Target: >0.90 para an√°lisis arquitect√≥nicos
```

### Indicadores de Calidad
- ‚úÖ File references: >30 en P4, >2 en P3
- ‚úÖ Technical terms: >100 en P4, >7 en P3
- ‚úÖ Code blocks: >30 en P4, >10 en P3
- ‚úÖ Tables: >20 en P4
- ‚úÖ Headers: >50 en P4 (estructura profesional)
```

2. **Actualizar `.claude/project/CLAUDE.md`** (secci√≥n nueva)

```markdown
## üìä Optimizaci√≥n de Prompts por Complejidad

Basado en validaci√≥n experimental (2025-11-11):

**P1 (Simple)**: Consultas factuales ‚Üí 100 palabras target
**P2 (Medio)**: An√°lisis t√©cnico ‚Üí 300-400 palabras target  
**P3 (Complejo)**: Multi-m√≥dulo ‚Üí 800-1,000 palabras target
**P4 (Cr√≠tico)**: Arquitect√≥nico ‚Üí 1,200-1,500 palabras target

**Ver detalles**: `.claude/project/PROMPTING_BEST_PRACTICES.md`
```

### C. Codex CLI (.codex/)

**Archivo a crear**: `.codex/prompting_guidelines.md`

```markdown
# Codex CLI - Gu√≠as de Prompting Efectivo

## Configuraci√≥n Recomendada

```yaml
# .codex/config.yml
model: o1-preview
temperature: 0.05  # M√°xima precisi√≥n para c√≥digo
max_tokens: 4000   # Suficiente para P4

prompt_templates:
  simple:
    target_words: 100
    format: "list"
  
  technical:
    target_words: 350
    format: "structured"
  
  architectural:
    target_words: 1300
    format: "professional_report"
```

## Prompts Validados para Codex

### An√°lisis de C√≥digo (P2-P3)
```
Analiza [archivo.py] l√≠neas [X-Y]:
- Patrones Odoo 19 (‚úÖ correcto / ‚ùå deprecated)
- Complejidad ciclom√°tica
- Vulnerabilidades potenciales
- Mejoras sugeridas con c√≥digo

Output esperado: 300-400 palabras, 3-5 code snippets
```

### Auditor√≠a Arquitect√≥nica (P4)
```
Audita arquitectura de [sistema]:
- Componentes: [lista con l√≠neas totales]
- Enfoque: seguridad, escalabilidad, mantenibilidad
- Formato: reporte profesional con tablas comparativas

Output esperado: 1,200+ palabras, especificidad >0.90
```
```

### D. Gemini CLI (.gemini/)

**Archivo a crear**: `.gemini/prompt_optimization.md`

```markdown
# Gemini CLI - Optimizaci√≥n de Prompts

## Gemini 2.5 Pro: Configuraci√≥n √ìptima

```yaml
# .gemini/settings.yml
model: gemini-2.5-pro
temperature: 0.1   # Balance precisi√≥n/creatividad
top_p: 0.95
top_k: 40

response_style:
  P1: "concise"      # 100 palabras target
  P2: "technical"    # 350 palabras target
  P3: "analytical"   # 900 palabras target
  P4: "comprehensive" # 1,300 palabras target
```

## Templates Espec√≠ficos Gemini

### P4 - An√°lisis Exhaustivo
```
Context: [Proyecto Odoo 19 CE, m√≥dulos chilenos, 50K+ l√≠neas]

Task: Eval√∫a arquitectura de [sistema migration] con enfoque en:
1. Decisiones de dise√±o y justificaciones
2. Patrones identificados (Strategy, Template Method, etc.)
3. Trade-offs cr√≠ticos (seguridad vs automatizaci√≥n)
4. Edge cases no cubiertos
5. Propuestas de mejora con c√≥digo

Formato: Reporte ejecutivo + an√°lisis t√©cnico profundo
Extensi√≥n: 1,200-1,500 palabras
Especificidad target: 0.90+
```

## Validaci√≥n de Outputs

Gemini 2.5 Pro scores esperados (basado en benchmark interno):
- Especificidad: 0.85-0.95 (P4)
- File references: 25-35 (P4)
- Technical terms: 80-120 (P4)
```

---

## üîß HERRAMIENTAS DE VALIDACI√ìN

### Script de An√°lisis de Respuestas

**Ubicaci√≥n**: `experimentos/analysis/analyze_response.py` (ya creado)

**Uso en desarrollo**:

```bash
# Analizar output de cualquier CLI
.venv/bin/python3 experimentos/analysis/analyze_response.py \
  outputs/my_analysis.txt \
  prompt_id \
  P3

# Output JSON con m√©tricas
{
  "words": 915,
  "specificity_score": 0.74,
  "file_references": 2,
  "technical_terms": 7,
  "style": "conversational"
}
```

### Integraci√≥n en Flujo de Trabajo

**Git Hook Pre-Commit** (opcional):

```bash
# .git/hooks/pre-commit
#!/bin/bash
# Validar prompts nuevos en docs/prompts_desarrollo/

for prompt in $(git diff --cached --name-only --diff-filter=A | grep "^docs/prompts_desarrollo/prompt_.*\.md$"); do
    echo "Validando prompt: $prompt"
    
    # Verificar que incluye nivel de complejidad (P1-P4)
    if ! grep -q "Nivel: P[1-4]" "$prompt"; then
        echo "‚ùå Prompt debe especificar nivel (P1-P4)"
        exit 1
    fi
    
    # Verificar que incluye output esperado
    if ! grep -q "Output esperado:" "$prompt"; then
        echo "‚ùå Prompt debe especificar output esperado"
        exit 1
    fi
done

echo "‚úÖ Prompts validados"
```

---

## üìñ DOCUMENTACI√ìN PARA DESARROLLADORES

### Archivo: `docs/guides/COMO_ESCRIBIR_PROMPTS_EFECTIVOS.md` (NUEVO)

```markdown
# C√≥mo Escribir Prompts Efectivos - Gu√≠a del Desarrollador

## Caso de Uso 1: Query R√°pida (P1)

**Cu√°ndo usar**: Necesitas dato factual r√°pido

**Template**:
```
[Verbo] + [Sustantivo concreto]
```

**Ejemplo**:
```
Lista m√≥dulos en addons/localization/
```

**Output esperado**: 70-150 palabras, lista simple

---

## Caso de Uso 2: An√°lisis de Archivo (P2)

**Cu√°ndo usar**: Necesitas entender un archivo espec√≠fico

**Template**:
```
Analiza [archivo] enfoc√°ndote en:
- [Aspecto 1]
- [Aspecto 2]
- [Aspecto 3]
```

**Ejemplo**:
```
Analiza account_move_dte.py enfoc√°ndote en:
- Patrones de herencia (@api decorators)
- Campos l10n_cl_* agregados
- M√©todos compute y validaciones
```

**Output esperado**: 300-400 palabras, 1 code snippet, tabla si aplica

---

## Caso de Uso 3: Comparaci√≥n Multi-M√≥dulo (P3)

**Cu√°ndo usar**: Necesitas comparar arquitecturas o identificar inconsistencias

**Template**:
```
Compara arquitectura de [m√≥dulo A], [m√≥dulo B], [m√≥dulo C]:

Dimensiones de an√°lisis:
1. [Dimensi√≥n 1]
2. [Dimensi√≥n 2]
3. [Dimensi√≥n 3]

Identifica:
- Patrones comunes
- Inconsistencias cr√≠ticas
- Mejores pr√°cticas aplicadas
```

**Ejemplo**:
```
Compara arquitectura de l10n_cl_dte, l10n_cl_hr_payroll, l10n_cl_financial_reports:

Dimensiones:
1. Patr√≥n de herencia (_inherit vs _name)
2. Naming conventions (dte_* vs l10n_cl_* vs sin prefijo)
3. Estrategia de parsing (libs/ vs AI-Service vs mixto)

Identifica inconsistencias que afecten mantenibilidad
```

**Output esperado**: 800-1,000 palabras, 10+ code snippets, 2+ file references

---

## Caso de Uso 4: An√°lisis Arquitect√≥nico (P4)

**Cu√°ndo usar**: Evaluaci√≥n profunda de sistema completo, decisiones de dise√±o

**Template**:
```
Analiza cr√≠ticamente la arquitectura de [sistema]:

**Contexto**: [Descripci√≥n detallada: N componentes, X l√≠neas totales]

**Eval√∫a**:
1. [Dimensi√≥n arquitect√≥nica 1] (separaci√≥n, flujo, patrones)
2. [Dimensi√≥n t√©cnica 2] (herramientas, edge cases, alternativas)
3. [Aspecto de seguridad] (capas, fallos, recuperaci√≥n)
4. [Validaci√≥n/Testing] (suficiencia, tipos, completitud)
5. [Performance] (cuellos de botella, escalabilidad)
6. [Trade-offs] (priorizaci√≥n de conflictos)
7. [Mejoras] (propuestas concretas con c√≥digo)

**Archivos a analizar**:
- [path/file1.py (N l√≠neas)]
- [path/file2.py (M l√≠neas)]
- [...]

**Entregable esperado**:
An√°lisis profesional que eval√∫e decisiones de dise√±o, fortalezas/debilidades,
riesgos identificados, recomendaciones con ejemplos de c√≥digo concretos
```

**Ejemplo real**:
```
Analiza cr√≠ticamente la arquitectura de sistema de migraci√≥n Odoo 19 CE:

Contexto: Sistema de 3 capas (Audit ‚Üí Migrate ‚Üí Validate), 2,723 l√≠neas,
137 migraciones autom√°ticas aplicadas, validaci√≥n triple, backups autom√°ticos

Eval√∫a:
1. Dise√±o de 3 capas (separaci√≥n adecuada, flujo datos, patrones detectados)
2. Estrategia parsing (AST Python vs regex vs XML ElementTree)
3. Sistema seguridad multi-capa (Git stash + backups + commits)
4. Validaci√≥n triple (sintaxis + sem√°ntica + funcional, suficiencia)
5. Escalabilidad (performance con 10K archivos)
6. Trade-offs (automatizaci√≥n vs seguridad, velocidad vs exhaustividad)
7. Mejoras cr√≠ticas (validaci√≥n JSON schema, paralelizaci√≥n, rollback)

Archivos:
- scripts/odoo19_migration/1_audit_deprecations.py (444 l√≠neas)
- scripts/odoo19_migration/2_migrate_safe.py (406 l√≠neas)
- scripts/odoo19_migration/3_validate_changes.py (455 l√≠neas)
- scripts/odoo19_migration/MASTER_ORCHESTRATOR.sh (414 l√≠neas)
- scripts/odoo19_migration/config/deprecations.yaml (284 l√≠neas)
```

**Output esperado**: 
- 1,200-1,500 palabras
- 30+ file references expl√≠citos (formato file.py:line)
- 100+ technical terms (AST, regex, Strategy Pattern, trade-off, etc.)
- 30+ code snippets (soluciones arquitect√≥nicas propuestas)
- 20+ tablas comparativas
- Especificidad >0.90 (m√°xima precisi√≥n t√©cnica)
- Estructura profesional (50+ headers multi-nivel)

---

## Validaci√≥n de Calidad

Usa `analyze_response.py` para validar outputs:

```bash
.venv/bin/python3 experimentos/analysis/analyze_response.py output.txt prompt_id P3
```

**Targets por nivel**:
- P1: words<200, specificity>0.50
- P2: words 300-400, specificity>0.55
- P3: words 800-1000, specificity>0.70, file_refs>2
- P4: words 1200-1500, specificity>0.90, file_refs>30, tech_terms>100
```

---

## üéì EDUCACI√ìN DEL EQUIPO

### 1. Sesi√≥n de Capacitaci√≥n (1 hora)

**Agenda**:
```
15 min: Presentaci√≥n hallazgos experimento
20 min: Demo en vivo P1‚ÜíP4 con m√©tricas
15 min: Workshop pr√°ctico (escribir prompts)
10 min: Q&A
```

**Materiales**:
- `experimentos/RESULTADOS_FINALES_P4.md` (reporte completo)
- `docs/guides/COMO_ESCRIBIR_PROMPTS_EFECTIVOS.md` (gu√≠a pr√°ctica)

### 2. Checklist de Onboarding

**Para nuevos desarrolladores**:

```markdown
## Checklist: Dominio de Prompting Efectivo

- [ ] Le√≠ `RESULTADOS_FINALES_P4.md` (hallazgos experimento)
- [ ] Revis√© templates P1-P4 en `EJEMPLOS_PROMPTS_POR_NIVEL.md`
- [ ] Practiqu√© con 1 prompt de cada nivel (P1, P2, P3, P4)
- [ ] Valid√© outputs con `analyze_response.py`
- [ ] Entiendo m√©tricas: especificidad, file refs, tech terms
- [ ] S√© cu√°ndo usar P1 vs P4 (complejidad del an√°lisis requerido)
```

---

## üìä M√âTRICAS DE ADOPCI√ìN

### KPIs a Trackear

```yaml
M√©tricas de Calidad de Prompts:
  - Especificidad promedio por nivel (target P4: >0.90)
  - File references en an√°lisis complejos (target P4: >30)
  - Densidad t√©cnica (target P4: >8 t√©rminos/100 palabras)

M√©tricas de Eficiencia:
  - Tiempo promedio an√°lisis P1: <1 min
  - Tiempo promedio an√°lisis P4: <5 min
  - Reducci√≥n de iteraciones (prompt ‚Üí output √∫til)

M√©tricas de Adopci√≥n:
  - % equipo usa templates por nivel
  - # prompts nuevos validados con analyze_response.py
  - % PRs con an√°lisis arquitect√≥nico P4 (si aplica)
```

### Dashboard de Prompting

**Ubicaci√≥n sugerida**: `docs/dashboards/prompting_metrics.md`

```markdown
# Dashboard: Calidad de Prompting

**√öltima actualizaci√≥n**: 2025-11-11

## Baseline (Experimento Inicial)

| Nivel | Palabras | Especificidad | File Refs | Tech Terms |
|-------|----------|---------------|-----------|------------|
| P1    | 100      | 0.53          | 0         | 0          |
| P2    | 342      | 0.56          | 0         | 0          |
| P3    | 915      | 0.74          | 2         | 7          |
| P4    | 1,303    | 0.95          | 31        | 109        |

## Prompts Producci√≥n (Tracking)

| Fecha | Prompt | Nivel | Palabras | Especificidad | Status |
|-------|--------|-------|----------|---------------|--------|
| ...   | ...    | ...   | ...      | ...           | ...    |
```

---

## üöÄ PLAN DE IMPLEMENTACI√ìN

### Fase 1: Documentaci√≥n (1-2 d√≠as)

- [x] Crear `ESTRATEGIA_PROMPTING_EFECTIVO.md`
- [ ] Crear `EJEMPLOS_PROMPTS_POR_NIVEL.md`
- [ ] Crear `METRICAS_CALIDAD_RESPUESTAS.md`
- [ ] Actualizar `README.md` en `docs/prompts_desarrollo/`
- [ ] Crear `docs/guides/COMO_ESCRIBIR_PROMPTS_EFECTIVOS.md`

### Fase 2: Propagaci√≥n a CLIs (2-3 d√≠as)

- [ ] Actualizar `.github/copilot-instructions.md`
- [ ] Crear `.claude/project/PROMPTING_BEST_PRACTICES.md`
- [ ] Crear `.codex/prompting_guidelines.md`
- [ ] Crear `.gemini/prompt_optimization.md`

### Fase 3: Herramientas (1 d√≠a)

- [x] Script `analyze_response.py` creado
- [ ] Integrar en CI/CD (opcional)
- [ ] Crear git hook validaci√≥n prompts (opcional)

### Fase 4: Educaci√≥n (1 semana)

- [ ] Sesi√≥n capacitaci√≥n equipo (1 hora)
- [ ] Actualizar checklist onboarding
- [ ] Crear dashboard m√©tricas

### Fase 5: Monitoreo (Continuo)

- [ ] Trackear KPIs de calidad prompts
- [ ] Revisar y actualizar templates mensualmente
- [ ] Recopilar feedback del equipo

---

## üìå PR√ìXIMOS PASOS INMEDIATOS

### Para Pedro (Hoy)

1. ‚úÖ Crear este documento de estrategia
2. ‚è≥ Revisar y aprobar estructura propuesta
3. ‚è≥ Decidir: ¬øimplementaci√≥n inmediata o gradual?

### Para Implementaci√≥n (Esta semana)

1. Crear archivos de documentaci√≥n (Fase 1)
2. Actualizar configuraciones de CLIs (Fase 2)
3. Programar sesi√≥n capacitaci√≥n equipo

---

## üí° COMENTARIOS Y REFLEXIONES

### ¬øPor qu√© esto es cr√≠tico?

**Antes del experimento**:
- Prompts gen√©ricos produc√≠an outputs inconsistentes
- No hab√≠a m√©tricas objetivas de calidad
- Variaci√≥n 4-6x entre CLIs sin explicaci√≥n clara

**Despu√©s del experimento**:
- **Entendemos el "por qu√©"** de la variaci√≥n (system prompt + complejidad)
- **Tenemos templates validados** para cada nivel de complejidad
- **M√©tricas cuantificables**: especificidad, file refs, tech terms
- **Configuraci√≥n CLI √≥ptima**: brevedad extrema P1, profundidad P4

### ¬øQu√© ganamos?

1. **Eficiencia**: Desarrolladores saben exactamente c√≥mo estructurar prompts
2. **Calidad**: Outputs predecibles seg√∫n nivel de complejidad
3. **Consistencia**: Todos los CLIs usan mismas convenciones
4. **Medici√≥n**: Podemos validar objetivamente calidad de an√°lisis

### ¬øRiesgos?

- ‚ö†Ô∏è **Sobreespecificaci√≥n**: Templates muy r√≠gidos pueden limitar creatividad
- ‚ö†Ô∏è **Mantenimiento**: Actualizar 4 CLIs al mismo tiempo requiere esfuerzo
- ‚ö†Ô∏è **Adopci√≥n**: Equipo debe aprender nuevas convenciones

**Mitigaci√≥n**:
- Templates son gu√≠as, no reglas estrictas
- Documentaci√≥n centralizada en `docs/prompts_desarrollo/`
- Sesi√≥n capacitaci√≥n + checklist onboarding

---

**Autor**: GitHub Copilot + Claude Sonnet 4.5  
**Basado en**: Experimento de locuacidad (6 prompts, 13x escalamiento validado)  
**Validado**: 2025-11-11

