# Auditoría Tests - AI Service Microservice

**ROL:** Agente Auditor Testing QA Expert

**OBJETIVO:** Auditar calidad y cobertura de tests, evaluar vs target 90% coverage, identificar edge cases faltantes

**MÓDULO EN ALCANCE:**
- `/Users/pedro/Documents/odoo19/ai-service/tests/`
- 20 archivos de tests (unit + integration)
- Target: 90% coverage

**CONTEXTO CRÍTICO:**
- Microservicio crítico para producción (DTE validation, payroll)
- 20+ endpoints REST que requieren tests
- Integraciones externas: Anthropic API, Redis, Odoo

**CRITERIOS DE AUDITORÍA:**

## 1. Cobertura de Tests

### 1.1 Coverage Actual vs Target
- **Target:** 90% line coverage
- ⚠️ Ejecutar pytest con coverage report
- ⚠️ Identificar módulos/archivos < 80% coverage
- ⚠️ Priorizar coverage de lógica crítica (validators, error handlers)

### 1.2 Tests por Módulo
- `tests/unit/` - Tests unitarios aislados
- `tests/integration/` - Tests de integración endpoints
- ⚠️ Verificar que TODOS los endpoints tienen tests
- ⚠️ Validar que TODOS los Pydantic validators tienen tests

## 2. Calidad de Tests

### 2.1 Test Structure
- ✅ Uso de fixtures (conftest.py)
- ✅ Tests async con pytest-asyncio
- ⚠️ Verificar que tests son independientes (NO state sharing)
- ⚠️ Validar que tests son deterministas (NO flaky tests)

### 2.2 Mocking
- ⚠️ Verificar que APIs externas están mockeadas (Anthropic, Redis, Previred)
- ⚠️ Validar que mocks son realistas (respuestas válidas)
- ⚠️ Verificar que NO hay tests que llaman APIs reales (costoso + lento)

### 2.3 Assertions
- ⚠️ Verificar que assertions son específicas (NO solo assert response.status == 200)
- ⚠️ Validar que se verifican response bodies completos
- ⚠️ Verificar que se validan side effects (logs, cache, DB)

## 3. Edge Cases y Error Paths

### 3.1 Happy Path vs Sad Path
- ⚠️ Verificar balance entre happy path (70%) y sad path (30%) tests
- ⚠️ Validar que se testean errores esperados (400, 401, 422, 500)
- ⚠️ Verificar que se testean timeouts y network errors

### 3.2 Edge Cases Críticos
- **RUT validation:** DV incorrecto, formato inválido, RUT muy largo
- **Montos:** negativos, cero, muy grandes (> 999 trillones), decimales
- **Fechas:** futuras, muy antiguas, formatos inválidos
- **Strings:** vacíos, muy largos (> max_length), caracteres especiales, SQL injection attempts
- **Lists:** vacíos, > max_items, con elementos inválidos

### 3.3 Boundary Conditions
- Rate limits: exactamente en el límite, just above, just below
- TTL: cache expiration edge cases
- Pagination: first page, last page, página inexistente

## 4. Test Performance

### 4.1 Velocidad
- ⚠️ Verificar que test suite completo corre en < 60 segundos
- ⚠️ Identificar tests lentos (> 5 segundos)
- ⚠️ Validar que NO hay sleeps innecesarios

### 4.2 Markers
- ✅ Uso de markers pytest (unit, integration, slow)
- ⚠️ Verificar que tests integration están marcados
- ⚠️ Validar que tests slow se pueden skip

## 5. Tests Faltantes Críticos

### 5.1 Endpoints sin Tests
- ⚠️ Identificar endpoints sin test coverage
- ⚠️ Priorizar endpoints críticos: `/api/ai/validate`, `/api/payroll/validate`

### 5.2 Validators sin Tests
- ⚠️ Verificar que `DTEValidationRequest.validate_dte_data()` tiene tests completos
- ⚠️ Verificar que `PayrollValidationRequest.validate_wage()` tiene edge cases
- ⚠️ Verificar que `ChatMessageRequest.validate_message()` tiene XSS/SQL injection tests

### 5.3 Error Handlers sin Tests
- ⚠️ Verificar que middleware error tracking tiene tests
- ⚠️ Validar que graceful degradation (cache failures) está testeado

**COMANDOS ÚTILES:**

```bash
cd /Users/pedro/Documents/odoo19/ai-service

# Coverage report
pytest --cov=. --cov-report=term-missing --cov-report=html tests/

# Tests lentos
pytest -v --durations=10 tests/

# Tests por marker
pytest -m "unit" tests/
pytest -m "integration" tests/

# Count tests
find tests/ -name "test_*.py" -exec grep -h "^def test_\|^async def test_" {} \; | wc -l
```

**ENTREGABLE:**

Generar `AUDIT_TESTS_AI_SERVICE_2025-11-13.md` con:

1. **Resumen Ejecutivo** (3-5 gaps críticos en testing)

2. **Score Tests:** [X]/100
   - Coverage (vs 90% target): [X]/30
   - Test Quality: [X]/25
   - Edge Cases: [X]/25
   - Performance: [X]/20

3. **Matriz de Hallazgos:**

| ID | Módulo/Endpoint | Gap Description | Criticidad | Tests Faltantes |
|----|----------------|-----------------|-----------|-----------------|
| T1 | validators.py | Missing edge cases | P0/P1/P2 | [list tests] |

4. **Métricas:**
   - Coverage actual: [X]%
   - Tests totales: [N]
   - Endpoints sin tests: [N]
   - Validators sin tests: [N]
   - Tests lentos (> 5s): [N]
   - Tests flaky detectados: [N]

5. **Tests Prioritarios a Agregar:**
   Lista top 10 tests más críticos faltantes

**OUTPUT FORMAT (OBLIGATORIO):**

```markdown
**Score:** [N]/100
**Coverage:** [X]% (Target: 90%)

**Fecha:** 2025-11-13
**Auditor:** Gemini CLI (Flash Pro)
**Módulo:** ai-service
**Dimensión:** Tests & Coverage

## Hallazgos

[P0] Critical test gap (module/file.py)
[P1] High priority missing tests (endpoint)
[P2] Medium priority edge cases (validator)
```

**RESTRICCIONES:**
- Modo solo lectura
- Ejecutar pytest para coverage real (NO estimaciones)
- Basar análisis en tests existentes
- Priorizar tests críticos para producción (DTE, payroll)
