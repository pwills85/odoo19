# CONTEXTO

Lee PRIMERO este archivo para entender tu rol y permisos:
`/Users/pedro/Documents/odoo19/docs/prompts/00_knowledge_base/CLI_AGENTS_SYSTEM_CONTEXT.md`

Eres un CLI agent (Copilot) trabajando bajo orquestaci√≥n de Claude Code Sonnet 4.5 (Orchestrator Maestro). Tu rol es ejecutar una auditor√≠a de Tests del microservicio ai-service de forma **100% aut√≥noma**.

---

# TAREA

Audita la cobertura y calidad de tests del microservicio **ai-service**.

**M√≥dulo:** `/Users/pedro/Documents/odoo19/ai-service/`

**Enfoque:**
- Coverage actual vs target 90%
- Test structure (unit vs integration)
- Edge cases coverage (validators, error paths)
- Test quality (mocking, fixtures, async)
- Missing tests (endpoints, validators, utils)
- Test execution speed

**Target Score:** 90/100 (Excelente)

---

# PERMISOS (Pre-Autorizados - NO PEDIR CONFIRMACI√ìN)

Seg√∫n CLI_AGENTS_SYSTEM_CONTEXT.md, tienes **autonom√≠a total** para:

‚úÖ **Lectura:**
- Full access a TODO el proyecto: `/Users/pedro/Documents/odoo19/**`
- Leer tests: `ai-service/tests/**/*.py`
- Leer c√≥digo source para entender qu√© deber√≠a testearse
- Analizar pytest configuration: `pytest.ini`, `conftest.py`

‚úÖ **Escritura:**
- Escribir reporte en: `docs/prompts/06_outputs/2025-11/AUDIT_TESTS_AI_SERVICE_V2_2025-11-13.md`

‚úÖ **An√°lisis:**
- Contar tests existentes vs endpoints
- Identificar validators sin edge cases
- Revisar fixtures y mocking patterns
- Estimar coverage bas√°ndote en estructura (NO ejecutar pytest coverage si es muy costoso)

‚ùå **NO Hacer:**
- Modificar tests existentes
- Ejecutar suite completa de tests (solo an√°lisis est√°tico)
- Crear nuevos tests

**IMPORTANTE:** NO pidas confirmaci√≥n para reads/an√°lisis/writes en rutas autorizadas.

---

# OUTPUT

**Archivo de Reporte:**
`/Users/pedro/Documents/odoo19/docs/prompts/06_outputs/2025-11/AUDIT_TESTS_AI_SERVICE_V2_2025-11-13.md`

**Formato del Reporte:**

```markdown
# Auditor√≠a Tests - AI Service Microservice

**Score:** X/100
**Fecha:** 2025-11-13
**Auditor:** Copilot CLI (GPT-4o)
**M√≥dulo:** ai-service
**Dimensi√≥n:** Tests (Coverage + Quality + Edge Cases)

---

## üìä Resumen Ejecutivo

[Overview de estado de tests]

### Hallazgos Cr√≠ticos (Top 3):
1. **[P1/P2/P3]** Coverage X% < 90% target (-X%)
2. **[P1/P2/P3]** X endpoints sin tests (X%)
3. **[P1/P2/P3]** Validators sin edge cases completos

---

## üéØ Score Breakdown

| Categor√≠a | Score | Detalles |
|-----------|-------|----------|
| **Coverage %** | X/25 | Actual vs 90% target |
| **Test Structure** | X/25 | Organization, fixtures, markers |
| **Edge Cases** | X/25 | Validators, error paths |
| **Test Quality** | X/25 | Mocking, async, execution speed |
| **TOTAL** | **X/100** | Grade: A/B/C/D |

---

## üîç Hallazgos Detallados

### Test-1: Coverage < 90% Target (P1 - High)
**Descripci√≥n:** Coverage actual es X%, target es 90% (-X% gap)

**M√≥dulos con coverage < 80%:**
- `main.py`: X% (esperado: 90%)
- `payroll/payroll_validator.py`: X% (esperado: 90%)
- `sii_monitor/orchestrator.py`: X% (esperado: 90%)

**L√≠neas no cubiertas:** ~X l√≠neas

**Recomendaci√≥n:**
Agregar ~X tests unitarios priorizando:
1. Happy paths de endpoints sin coverage
2. Error paths (try/except blocks)
3. Edge cases de validators

**Esfuerzo:** X-X horas

---

### Test-2: Endpoints Sin Tests (P2 - Medium)
**Descripci√≥n:** X/Y endpoints (X%) sin integration tests

**Endpoints faltantes:**
- `/api/ai/sii/monitor` (P1)
- `/metrics/costs` (P2)
- [... lista completa ...]

**Recomendaci√≥n:**
Agregar integration tests para cada endpoint con:
- Happy path (200 OK)
- Error cases (400, 401, 422, 500)
- Edge cases (payload limits, timeout simulation)

**Esfuerzo:** X horas (X min por endpoint)

---

### Test-3: Validators Sin Edge Cases (P2 - Medium)
**Descripci√≥n:** Validators tienen tests b√°sicos pero faltan edge cases

**Validators afectados:**
- `validate_rut()`: falta test para RUT frontera (X-Y)
- `validate_monto()`: falta test para montos negativos, 0, max int
- `validate_chat_messages()`: falta test para >100 mensajes, unicode

**Recomendaci√≥n:**
```python
# Ejemplo: tests/unit/test_validators_edge_cases.py
def test_validate_rut_edge_cases():
    assert validate_rut("1-9")  # Min RUT
    assert validate_rut("99999999-9")  # Max RUT
    with pytest.raises(ValidationError):
        validate_rut("0-0")  # Invalid
```

**Esfuerzo:** X horas

---

[... M√°s hallazgos ...]

---

## üìä M√©tricas Tests

| M√©trica | Valor | Target | Gap |
|---------|-------|--------|-----|
| **Coverage %** | X% | 90% | -X% |
| **Tests unitarios** | X | ~Y (estimado) | -X |
| **Tests integraci√≥n** | X | Y (100% endpoints) | -X |
| **Endpoints con tests** | X/Y (X%) | Y/Y (100%) | -X |
| **Validators con edge cases** | X/Y (X%) | Y/Y (100%) | -X |
| **Execution time** | X.Xs | < 30s | OK/‚ö†Ô∏è |

---

## ‚úÖ Fortalezas Tests

- ‚úÖ Test structure bien organizado (unit/ vs integration/)
- ‚úÖ Fixtures reusables (conftest.py)
- ‚úÖ Async tests con pytest-asyncio
- ‚úÖ Markers configurados (unit, integration, slow)

---

## üöÄ Plan de Acci√≥n Prioritario

### Prioridad P1 (Alta)
1. Incrementar coverage X% ‚Üí 90% (~X tests nuevos, X-X horas)

### Prioridad P2 (Media)
1. Tests para X endpoints faltantes (X horas)
2. Edge cases validators (X horas)

**Esfuerzo Total P1+P2:** ~X-X horas

---

**CONCLUSI√ìN:** [Resumen coverage, gaps cr√≠ticos, esfuerzo para 90/100]
```

**Stdout (Retornar al Finalizar):**

```
‚úÖ Auditor√≠a Tests completada
Score: X/100 (Grade: A/B/C)
Coverage: X% (target: 90%, gap: -X%)

Top 3 Findings:
[P1] Coverage < 90% (-X%)
[P2] X endpoints sin tests (X%)
[P2] Validators sin edge cases

Tests faltantes: ~X
Esfuerzo: ~X-X horas
Reporte completo: docs/prompts/06_outputs/2025-11/AUDIT_TESTS_AI_SERVICE_V2_2025-11-13.md
```

---

# RESTRICCIONES

- ‚ùå NO modificar tests
- ‚ùå NO crear nuevos tests
- ‚è±Ô∏è  Target: < 5 minutos ejecuci√≥n

---

# NOTAS ADICIONALES

**Orquestaci√≥n v1.1 LEAN:**
- Claude NO lee tus logs (file polling)
- Escribe TODO en el archivo de reporte
- Primeras 50 l√≠neas = Score + Coverage % + Top 3 findings

**Archivos Clave:**
- `ai-service/tests/unit/**/*.py` (tests unitarios)
- `ai-service/tests/integration/**/*.py` (tests integraci√≥n)
- `ai-service/tests/conftest.py` (fixtures)
- `ai-service/pytest.ini` (config)
- `ai-service/main.py` (endpoints a testear)
- `ai-service/chat/validators.py` (validators a validar)

**Coverage Target Breakdown:**
- `main.py`: 90%+ (endpoints, middleware)
- `clients/`: 85%+ (API integration, mocking)
- `chat/`: 90%+ (engine, validators)
- `plugins/`: 80%+ (cada plugin)
- `utils/`: 95%+ (helpers, cache)

**Si Puedes Ejecutar Coverage R√°pido (<30s):**
```bash
cd ai-service
pytest --cov=. --cov-report=term-missing tests/ -q
# Incluir output en reporte
```

**Si Es Muy Costoso:**
Estima coverage bas√°ndote en:
- N√∫mero de tests vs l√≠neas de c√≥digo
- Endpoints con/sin tests
- Validators con/sin edge cases

¬°Adelante! Ejecuta la auditor√≠a de tests de forma completamente aut√≥noma.
