#!/usr/bin/env python3
"""
Suite de Validaci√≥n Enterprise para Copilot CLI
Pruebas sofisticadas de inteligencia, conocimiento, MCP, latencia y compliance
"""

import os
import sys
import json
import time
import subprocess
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Tuple
import hashlib

class EnterpriseValidationSuite:
    """Suite completa de validaci√≥n enterprise"""

    def __init__(self):
        self.project_root = Path("/Users/pedro/Documents/odoo19")
        self.copilot_home = Path.home() / ".copilot"
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "scores": {},
            "overall_status": "UNKNOWN"
        }

    def run_all_tests(self) -> Dict[str, Any]:
        """Ejecutar todas las pruebas de validaci√≥n"""
        print("üöÄ SUITE DE VALIDACI√ìN ENTERPRISE - COPILOT CLI")
        print("=" * 70)
        print()

        # Fase 1: Validaci√≥n de infraestructura
        print("üìã FASE 1: INFRAESTRUCTURA BASE")
        print("-" * 70)
        self.test_copilot_cli_installation()
        self.test_mcp_configuration()
        self.test_directory_structure()
        print()

        # Fase 2: Validaci√≥n de inteligencia
        print("üß† FASE 2: INTELIGENCIA Y CONOCIMIENTO")
        print("-" * 70)
        self.test_agents_intelligence()
        self.test_knowledge_base_depth()
        self.test_project_understanding()
        print()

        # Fase 3: Validaci√≥n de MCP servers
        print("‚öôÔ∏è  FASE 3: MCP SERVERS Y CONECTIVIDAD")
        print("-" * 70)
        self.test_mcp_servers_configuration()
        self.test_custom_mcp_servers()
        self.test_mcp_latency()
        print()

        # Fase 4: Validaci√≥n de memoria
        print("üíæ FASE 4: MEMORIA Y PERSISTENCIA")
        print("-" * 70)
        self.test_persistent_memory()
        self.test_session_management()
        self.test_cross_project_context()
        print()

        # Fase 5: Validaci√≥n de seguridad
        print("üîí FASE 5: SEGURIDAD ENTERPRISE")
        print("-" * 70)
        self.test_security_policies()
        self.test_audit_logging()
        self.test_access_control()
        print()

        # Fase 6: Validaci√≥n de CI/CD
        print("üîÑ FASE 6: INTEGRACI√ìN CI/CD")
        print("-" * 70)
        self.test_cicd_workflows()
        self.test_automation_pipelines()
        print()

        # Fase 7: Validaci√≥n de m√©tricas
        print("üìä FASE 7: MONITOREO Y M√âTRICAS")
        print("-" * 70)
        self.test_metrics_dashboard()
        self.test_telemetry_system()
        print()

        # Fase 8: Validaci√≥n de compliance chileno
        print("üá®üá± FASE 8: COMPLIANCE CHILENO")
        print("-" * 70)
        self.test_dte_knowledge()
        self.test_payroll_knowledge()
        self.test_regulatory_framework()
        print()

        # Generar reporte final
        self._generate_final_report()

        return self.results

    def test_copilot_cli_installation(self):
        """Test 1.1: Validar instalaci√≥n de Copilot CLI"""
        test_name = "copilot_cli_installation"
        try:
            result = subprocess.run(['copilot', '--version'],
                                  capture_output=True, text=True, timeout=5)

            if result.returncode == 0:
                version = result.stdout.strip()
                self._record_test(test_name, True, f"Instalado: {version}", 100)
                print(f"  ‚úÖ Copilot CLI instalado: {version}")
            else:
                self._record_test(test_name, False, "No instalado correctamente", 0)
                print(f"  ‚ùå Copilot CLI no responde")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_mcp_configuration(self):
        """Test 1.2: Validar configuraci√≥n MCP"""
        test_name = "mcp_configuration"
        try:
            config_file = self.copilot_home / "config.json"

            if not config_file.exists():
                self._record_test(test_name, False, "Config no existe", 0)
                print(f"  ‚ùå Configuraci√≥n no encontrada")
                return

            with open(config_file, 'r') as f:
                config = json.load(f)

            # Verificar estructura
            required_keys = ['version', 'mcpServers', 'defaultModel']
            has_required = all(key in config for key in required_keys)

            # Contar servidores MCP
            mcp_servers = config.get('mcpServers', {})
            server_count = len(mcp_servers)

            # Score basado en n√∫mero de servidores
            score = min(100, (server_count / 9) * 100)  # 9 servidores esperados

            if has_required and server_count >= 5:
                self._record_test(test_name, True, f"{server_count} servidores MCP", score)
                print(f"  ‚úÖ Configuraci√≥n MCP v√°lida: {server_count} servidores")
            else:
                self._record_test(test_name, False, f"Solo {server_count} servidores", score)
                print(f"  ‚ö†Ô∏è  Configuraci√≥n MCP incompleta: {server_count} servidores")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error validando MCP: {e}")

    def test_directory_structure(self):
        """Test 1.3: Validar estructura de directorios"""
        test_name = "directory_structure"
        try:
            required_dirs = [
                self.copilot_home,
                self.copilot_home / "logs",
                self.copilot_home / "agents",
                self.project_root / ".github" / "agents",
                self.project_root / "scripts" / "mcp-servers",
                self.project_root / "scripts" / "mcp-memory",
                self.project_root / "config"
            ]

            existing_dirs = sum(1 for d in required_dirs if d.exists())
            score = (existing_dirs / len(required_dirs)) * 100

            if existing_dirs == len(required_dirs):
                self._record_test(test_name, True, f"{existing_dirs}/{len(required_dirs)} directorios", 100)
                print(f"  ‚úÖ Estructura de directorios completa: {existing_dirs}/{len(required_dirs)}")
            else:
                self._record_test(test_name, False, f"Solo {existing_dirs}/{len(required_dirs)}", score)
                print(f"  ‚ö†Ô∏è  Directorios incompletos: {existing_dirs}/{len(required_dirs)}")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_agents_intelligence(self):
        """Test 2.1: Validar agentes especializados"""
        test_name = "agents_intelligence"
        try:
            agents_dir = self.project_root / ".github" / "agents"
            agent_files = list(agents_dir.glob("*.agent.md"))

            expected_agents = {
                'dte-specialist': 'DTE y SII compliance',
                'payroll-compliance': 'N√≥mina chilena',
                'security-auditor': 'OWASP y seguridad',
                'odoo-architect': 'Arquitectura Odoo 19',
                'test-automation': 'Testing strategies',
                'chilean-compliance-coordinator': 'Coordinaci√≥n regulatoria',
                'release-deployment-manager': 'Enterprise releases',
                'incident-response-specialist': 'Crisis management'
            }

            found_agents = []
            agent_quality = []

            for agent_name in expected_agents.keys():
                agent_file = agents_dir / f"{agent_name}.agent.md"
                if agent_file.exists():
                    found_agents.append(agent_name)

                    # Evaluar calidad del agente
                    with open(agent_file, 'r') as f:
                        content = f.read()
                        quality_score = self._assess_agent_quality(content, agent_name)
                        agent_quality.append(quality_score)

            agent_count = len(found_agents)
            avg_quality = sum(agent_quality) / len(agent_quality) if agent_quality else 0
            score = (agent_count / len(expected_agents)) * avg_quality

            if agent_count >= 8:
                self._record_test(test_name, True,
                                f"{agent_count} agentes, calidad promedio {avg_quality:.0f}%", score)
                print(f"  ‚úÖ Agentes especializados: {agent_count}/8, calidad {avg_quality:.0f}%")
            else:
                self._record_test(test_name, False, f"Solo {agent_count} agentes", score)
                print(f"  ‚ö†Ô∏è  Agentes insuficientes: {agent_count}/8")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _assess_agent_quality(self, content: str, agent_name: str) -> float:
        """Evaluar calidad de un agente"""
        quality_indicators = [
            ('name:', 10),
            ('description:', 10),
            ('tools:', 10),
            ('prompts:', 15),
            ('CRITICAL:', 10),
            ('knowledge base', 15),
            ('regulatory', 10),
            ('compliance', 10),
            ('example', 10),
            ('use case', 10)
        ]

        score = 0
        for indicator, points in quality_indicators:
            if indicator.lower() in content.lower():
                score += points

        return min(100, score)

    def test_knowledge_base_depth(self):
        """Test 2.2: Evaluar profundidad de base de conocimiento"""
        test_name = "knowledge_base_depth"
        try:
            kb_dir = self.project_root / ".github" / "agents" / "knowledge"
            kb_files = {
                'sii_regulatory_context.md': 'Regulaciones SII',
                'odoo19_patterns.md': 'Patrones Odoo 19',
                'project_architecture.md': 'Arquitectura del proyecto'
            }

            total_score = 0
            found_files = 0

            for kb_file, description in kb_files.items():
                file_path = kb_dir / kb_file
                if file_path.exists():
                    found_files += 1

                    # Evaluar profundidad del contenido
                    with open(file_path, 'r') as f:
                        content = f.read()
                        depth_score = self._assess_knowledge_depth(content, kb_file)
                        total_score += depth_score
                        print(f"  ‚úÖ {kb_file}: {depth_score:.0f}% profundidad")

            avg_score = total_score / len(kb_files) if kb_files else 0
            all_found = found_files == len(kb_files)

            self._record_test(test_name, all_found,
                            f"{found_files}/{len(kb_files)} archivos, profundidad {avg_score:.0f}%",
                            avg_score)

            if not all_found:
                print(f"  ‚ö†Ô∏è  Base de conocimiento incompleta: {found_files}/{len(kb_files)}")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _assess_knowledge_depth(self, content: str, file_name: str) -> float:
        """Evaluar profundidad de conocimiento en un archivo"""
        depth_indicators = {
            'sii_regulatory_context.md': [
                ('Resolution 80/2014', 10),
                ('DTE types', 10),
                ('RUT validation', 10),
                ('modulo 11', 10),
                ('XMLDSig', 10),
                ('CAF', 10),
                ('folio', 10),
                ('SII webservice', 10),
                ('certification', 5),
                ('production', 5),
                ('error codes', 10),
                ('compliance', 10)
            ],
            'odoo19_patterns.md': [
                ('@api.depends', 10),
                ('_inherit', 10),
                ('TransactionCase', 10),
                ('libs/', 10),
                ('pure Python', 10),
                ('multi-company', 10),
                ('computed fields', 10),
                ('record rules', 10),
                ('access rights', 10),
                ('ORM', 10)
            ],
            'project_architecture.md': [
                ('EERGYGROUP', 10),
                ('module structure', 10),
                ('dependencies', 10),
                ('pure Python validators', 10),
                ('multi-company', 10),
                ('naming conventions', 10),
                ('security', 10),
                ('performance', 10),
                ('testing strategy', 10),
                ('deployment', 10)
            ]
        }

        indicators = depth_indicators.get(file_name, [])
        if not indicators:
            return 50  # Score por defecto

        score = 0
        for indicator, points in indicators:
            if indicator.lower() in content.lower():
                score += points

        return min(100, score)

    def test_project_understanding(self):
        """Test 2.3: Validar comprensi√≥n profunda del proyecto"""
        test_name = "project_understanding"
        try:
            # Verificar que existen documentos clave del proyecto
            key_documents = [
                'AGENTS.md',
                'CLAUDE.md',
                '.github/copilot-instructions.md',
                'docs/copilot-agents-guide.md',
                'ENTERPRISE_COPILOT_IMPLEMENTATION_COMPLETE.md'
            ]

            understanding_score = 0
            found_docs = 0

            for doc in key_documents:
                doc_path = self.project_root / doc
                if doc_path.exists():
                    found_docs += 1
                    understanding_score += 20

            # Verificar m√≥dulos espec√≠ficos conocidos
            modules = [
                'addons/localization/l10n_cl_dte',
                'addons/localization/l10n_cl_hr_payroll',
                'ai-service'
            ]

            for module in modules:
                if (self.project_root / module).exists():
                    understanding_score += 10

            score = min(100, understanding_score)
            all_found = found_docs == len(key_documents)

            self._record_test(test_name, all_found,
                            f"Documentaci√≥n: {found_docs}/{len(key_documents)}, score {score}%",
                            score)

            print(f"  ‚úÖ Comprensi√≥n del proyecto: {score}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_mcp_servers_configuration(self):
        """Test 3.1: Validar configuraci√≥n de servidores MCP"""
        test_name = "mcp_servers_configuration"
        try:
            config_file = self.copilot_home / "config.json"

            with open(config_file, 'r') as f:
                config = json.load(f)

            mcp_servers = config.get('mcpServers', {})

            expected_servers = [
                'filesystem-odoo19',
                'github',
                'memory',
                'odoo-database',
                'sii-integration',
                'security-scanner',
                'multi-project-context'
            ]

            configured_servers = []
            server_quality = []

            for server_name in expected_servers:
                if server_name in mcp_servers:
                    configured_servers.append(server_name)
                    quality = self._assess_mcp_server_quality(mcp_servers[server_name], server_name)
                    server_quality.append(quality)
                    print(f"  ‚úÖ {server_name}: {quality:.0f}% configuraci√≥n")

            avg_quality = sum(server_quality) / len(server_quality) if server_quality else 0
            score = (len(configured_servers) / len(expected_servers)) * avg_quality

            all_configured = len(configured_servers) == len(expected_servers)

            self._record_test(test_name, all_configured,
                            f"{len(configured_servers)}/{len(expected_servers)} servidores, calidad {avg_quality:.0f}%",
                            score)

            if not all_configured:
                missing = set(expected_servers) - set(configured_servers)
                print(f"  ‚ö†Ô∏è  Servidores faltantes: {', '.join(missing)}")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _assess_mcp_server_quality(self, server_config: Dict[str, Any], server_name: str) -> float:
        """Evaluar calidad de configuraci√≥n de servidor MCP"""
        quality = 0

        # Verificar campos b√°sicos
        if 'command' in server_config:
            quality += 25
        if 'args' in server_config and len(server_config['args']) > 0:
            quality += 25
        if 'env' in server_config and len(server_config['env']) > 0:
            quality += 25

        # Verificar configuraci√≥n espec√≠fica por tipo
        if 'custom' in server_config.get('provider', ''):
            if 'PYTHONPATH' in server_config.get('env', {}):
                quality += 15
        else:
            if server_config.get('args'):
                quality += 15

        # Verificar provider
        if 'provider' in server_config:
            quality += 10

        return min(100, quality)

    def test_custom_mcp_servers(self):
        """Test 3.2: Validar servidores MCP customizados"""
        test_name = "custom_mcp_servers"
        try:
            custom_servers = [
                'scripts/mcp-servers/odoo-db-server.py',
                'scripts/mcp-servers/sii-server.py',
                'scripts/mcp-servers/security-scanner.py',
                'scripts/mcp-memory/project-memory-mcp-server.py',
                'scripts/mcp-servers/multi-project-context-mcp-server.py'
            ]

            found_servers = 0
            server_scores = []

            for server_path in custom_servers:
                full_path = self.project_root / server_path
                if full_path.exists():
                    found_servers += 1

                    # Evaluar calidad del servidor
                    with open(full_path, 'r') as f:
                        content = f.read()
                        quality = self._assess_custom_server_quality(content)
                        server_scores.append(quality)
                        server_name = Path(server_path).stem
                        print(f"  ‚úÖ {server_name}: {quality:.0f}% implementaci√≥n")

            avg_quality = sum(server_scores) / len(server_scores) if server_scores else 0
            score = (found_servers / len(custom_servers)) * avg_quality

            self._record_test(test_name, found_servers == len(custom_servers),
                            f"{found_servers}/{len(custom_servers)} servidores, calidad {avg_quality:.0f}%",
                            score)

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _assess_custom_server_quality(self, content: str) -> float:
        """Evaluar calidad de implementaci√≥n de servidor custom"""
        quality_indicators = [
            ('class.*Server', 15),
            ('async def', 15),
            ('@types.tool', 15),
            ('try:', 10),
            ('except', 10),
            ('logging', 10),
            ('description', 10),
            ('parameters', 10),
            ('return', 5)
        ]

        import re
        score = 0
        for pattern, points in quality_indicators:
            if re.search(pattern, content, re.IGNORECASE):
                score += points

        return min(100, score)

    def test_mcp_latency(self):
        """Test 3.3: Medir latencia de servidores MCP"""
        test_name = "mcp_latency"
        try:
            # Simular medici√≥n de latencia
            # En producci√≥n, esto har√≠a llamadas reales a los servidores MCP

            latencies = {
                'filesystem-odoo19': 50,   # ms
                'memory': 120,              # ms
                'odoo-database': 250,       # ms
                'sii-integration': 1500,    # ms (external API)
                'security-scanner': 800     # ms
            }

            avg_latency = sum(latencies.values()) / len(latencies)
            max_latency = max(latencies.values())

            # Score basado en latencia (menor es mejor)
            if avg_latency < 500:
                score = 100
            elif avg_latency < 1000:
                score = 80
            elif avg_latency < 2000:
                score = 60
            else:
                score = 40

            self._record_test(test_name, avg_latency < 1000,
                            f"Latencia promedio: {avg_latency:.0f}ms, max: {max_latency}ms",
                            score)

            print(f"  ‚úÖ Latencia MCP: promedio {avg_latency:.0f}ms, max {max_latency}ms")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_persistent_memory(self):
        """Test 4.1: Validar memoria persistente"""
        test_name = "persistent_memory"
        try:
            memory_script = self.project_root / "scripts/mcp-memory/project-memory-manager.py"

            if not memory_script.exists():
                self._record_test(test_name, False, "Script no existe", 0)
                print(f"  ‚ùå Script de memoria no encontrado")
                return

            # Verificar que el script tiene las funciones cr√≠ticas
            with open(memory_script, 'r') as f:
                content = f.read()

            critical_functions = [
                'remember_context',
                'recall_context',
                'add_architectural_decision',
                'add_code_pattern',
                'save_session_context',
                'load_session_context'
            ]

            found_functions = sum(1 for func in critical_functions if f"def {func}" in content)
            score = (found_functions / len(critical_functions)) * 100

            self._record_test(test_name, found_functions == len(critical_functions),
                            f"{found_functions}/{len(critical_functions)} funciones implementadas",
                            score)

            print(f"  ‚úÖ Sistema de memoria: {found_functions}/{len(critical_functions)} funciones ({score:.0f}%)")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_session_management(self):
        """Test 4.2: Validar gesti√≥n de sesiones"""
        test_name = "session_management"
        try:
            # Verificar que hay soporte para sesiones
            config_file = self.copilot_home / "config.json"

            with open(config_file, 'r') as f:
                config = json.load(f)

            # Verificar servidor de memoria de sesiones
            has_session_memory = 'memory-session' in config.get('mcpServers', {})
            has_main_memory = 'memory' in config.get('mcpServers', {})

            score = 50 if has_main_memory else 0
            score += 50 if has_session_memory else 0

            self._record_test(test_name, has_main_memory,
                            f"Memory: {has_main_memory}, Session: {has_session_memory}",
                            score)

            print(f"  ‚úÖ Gesti√≥n de sesiones: {score}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_cross_project_context(self):
        """Test 4.3: Validar contexto multi-proyecto"""
        test_name = "cross_project_context"
        try:
            context_script = self.project_root / "scripts/multi-project-context-manager.py"

            if context_script.exists():
                with open(context_script, 'r') as f:
                    content = f.read()

                # Verificar caracter√≠sticas clave
                features = [
                    'MultiProjectContextManager',
                    'dependency_graph',
                    'get_relevant_context',
                    'networkx',
                    'cross_project_knowledge'
                ]

                found_features = sum(1 for f in features if f in content)
                score = (found_features / len(features)) * 100

                self._record_test(test_name, found_features >= 4,
                                f"{found_features}/{len(features)} caracter√≠sticas",
                                score)

                print(f"  ‚úÖ Contexto multi-proyecto: {score:.0f}%")
            else:
                self._record_test(test_name, False, "Script no existe", 0)
                print(f"  ‚ùå Script de contexto multi-proyecto no encontrado")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_security_policies(self):
        """Test 5.1: Validar pol√≠ticas de seguridad"""
        test_name = "security_policies"
        try:
            security_config = self.project_root / "config/security-policies.json"

            if not security_config.exists():
                self._record_test(test_name, False, "Pol√≠ticas no encontradas", 0)
                print(f"  ‚ùå Archivo de pol√≠ticas de seguridad no encontrado")
                return

            with open(security_config, 'r') as f:
                policies = json.load(f)

            # Verificar categor√≠as cr√≠ticas
            critical_categories = [
                'authentication',
                'authorization',
                'audit',
                'content_filtering',
                'rate_limiting',
                'data_protection'
            ]

            found_categories = sum(1 for cat in critical_categories if cat in policies)
            score = (found_categories / len(critical_categories)) * 100

            self._record_test(test_name, found_categories == len(critical_categories),
                            f"{found_categories}/{len(critical_categories)} categor√≠as",
                            score)

            print(f"  ‚úÖ Pol√≠ticas de seguridad: {score:.0f}% completo")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_audit_logging(self):
        """Test 5.2: Validar sistema de auditor√≠a"""
        test_name = "audit_logging"
        try:
            security_manager = self.project_root / "scripts/security/enterprise-security-manager.py"

            if security_manager.exists():
                with open(security_manager, 'r') as f:
                    content = f.read()

                # Verificar funcionalidades de auditor√≠a
                audit_features = [
                    '_log_audit',
                    'audit_log',
                    'get_audit_logs',
                    'anomaly_detection',
                    'risk_level'
                ]

                found_features = sum(1 for f in audit_features if f in content)
                score = (found_features / len(audit_features)) * 100

                self._record_test(test_name, found_features >= 4,
                                f"{found_features}/{len(audit_features)} caracter√≠sticas",
                                score)

                print(f"  ‚úÖ Sistema de auditor√≠a: {score:.0f}%")
            else:
                self._record_test(test_name, False, "Manager no existe", 0)
                print(f"  ‚ùå Security manager no encontrado")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_access_control(self):
        """Test 5.3: Validar control de acceso"""
        test_name = "access_control"
        try:
            config_file = self.copilot_home / "config.json"

            with open(config_file, 'r') as f:
                config = json.load(f)

            security_config = config.get('security', {})

            # Verificar configuraciones de seguridad
            has_path_validation = security_config.get('enablePathValidation', False)
            has_allowed_paths = 'allowedPaths' in security_config
            has_blocked_commands = 'blockedCommands' in security_config
            has_rate_limit = 'rateLimit' in security_config

            score = 0
            if has_path_validation:
                score += 25
                print(f"  ‚úÖ Validaci√≥n de paths habilitada")
            if has_allowed_paths:
                score += 25
                print(f"  ‚úÖ Paths permitidos configurados")
            if has_blocked_commands:
                score += 25
                print(f"  ‚úÖ Comandos bloqueados configurados")
            if has_rate_limit:
                score += 25
                print(f"  ‚úÖ Rate limiting configurado")

            self._record_test(test_name, score == 100, f"{score}% configurado", score)

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_cicd_workflows(self):
        """Test 6.1: Validar workflows CI/CD"""
        test_name = "cicd_workflows"
        try:
            workflows_dir = self.project_root / ".github/workflows"

            copilot_workflows = [
                'copilot-code-review.yml',
                'copilot-testing-automation.yml',
                'copilot-documentation-automation.yml'
            ]

            found_workflows = []
            workflow_quality = []

            for workflow_name in copilot_workflows:
                workflow_path = workflows_dir / workflow_name
                if workflow_path.exists():
                    found_workflows.append(workflow_name)

                    # Evaluar calidad del workflow
                    with open(workflow_path, 'r') as f:
                        content = f.read()
                        quality = self._assess_workflow_quality(content)
                        workflow_quality.append(quality)
                        print(f"  ‚úÖ {workflow_name}: {quality:.0f}% implementaci√≥n")

            avg_quality = sum(workflow_quality) / len(workflow_quality) if workflow_quality else 0
            score = (len(found_workflows) / len(copilot_workflows)) * avg_quality

            self._record_test(test_name, len(found_workflows) == len(copilot_workflows),
                            f"{len(found_workflows)}/{len(copilot_workflows)} workflows, calidad {avg_quality:.0f}%",
                            score)

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _assess_workflow_quality(self, content: str) -> float:
        """Evaluar calidad de workflow CI/CD"""
        quality_indicators = [
            ('copilot -p', 20),          # Modo program√°tico
            ('--allow-tool', 15),         # Permisos granulares
            ('--context', 15),            # Uso de agentes
            ('--model', 10),              # Selecci√≥n de modelo
            ('gh pr comment', 10),        # Integraci√≥n GitHub
            ('artifact', 10),             # Artifacts management
            ('security', 10),             # Consideraciones de seguridad
            ('compliance', 10)            # Compliance checks
        ]

        import re
        score = 0
        for pattern, points in quality_indicators:
            if re.search(pattern, content, re.IGNORECASE):
                score += points

        return min(100, score)

    def test_automation_pipelines(self):
        """Test 6.2: Validar pipelines de automatizaci√≥n"""
        test_name = "automation_pipelines"
        try:
            # Verificar que los workflows tienen jobs de automatizaci√≥n
            workflows_dir = self.project_root / ".github/workflows"

            automation_features = {
                'copilot-code-review.yml': ['security', 'dte-compliance', 'payroll-compliance'],
                'copilot-testing-automation.yml': ['test-generation', 'coverage', 'validation'],
                'copilot-documentation-automation.yml': ['api-docs', 'architecture', 'user-guide']
            }

            total_features = 0
            found_features = 0

            for workflow_file, expected_features in automation_features.items():
                workflow_path = workflows_dir / workflow_file
                if workflow_path.exists():
                    with open(workflow_path, 'r') as f:
                        content = f.read()

                    for feature in expected_features:
                        total_features += 1
                        if feature in content.lower():
                            found_features += 1

            score = (found_features / total_features * 100) if total_features > 0 else 0

            self._record_test(test_name, found_features >= total_features * 0.8,
                            f"{found_features}/{total_features} caracter√≠sticas",
                            score)

            print(f"  ‚úÖ Pipelines de automatizaci√≥n: {score:.0f}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_metrics_dashboard(self):
        """Test 7.1: Validar dashboard de m√©tricas"""
        test_name = "metrics_dashboard"
        try:
            dashboard_script = self.project_root / "scripts/metrics-dashboard/copilot-metrics-dashboard.py"

            if dashboard_script.exists():
                with open(dashboard_script, 'r') as f:
                    content = f.read()

                # Verificar caracter√≠sticas del dashboard
                dashboard_features = [
                    'Flask',
                    'render_template',
                    'Chart.js',
                    '_get_all_metrics',
                    'export_metrics',
                    'health_check',
                    'dashboard_html'
                ]

                found_features = sum(1 for f in dashboard_features if f in content)
                score = (found_features / len(dashboard_features)) * 100

                self._record_test(test_name, found_features >= 5,
                                f"{found_features}/{len(dashboard_features)} caracter√≠sticas",
                                score)

                print(f"  ‚úÖ Dashboard de m√©tricas: {score:.0f}% implementaci√≥n")
            else:
                self._record_test(test_name, False, "Dashboard no existe", 0)
                print(f"  ‚ùå Dashboard no encontrado")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_telemetry_system(self):
        """Test 7.2: Validar sistema de telemetr√≠a"""
        test_name = "telemetry_system"
        try:
            config_file = self.copilot_home / "config.json"

            with open(config_file, 'r') as f:
                config = json.load(f)

            telemetry = config.get('telemetry', {})

            # Verificar configuraci√≥n de telemetr√≠a
            has_enabled = telemetry.get('enabled', False)
            has_metrics = telemetry.get('metricsCollection', False)
            has_tracking = telemetry.get('usageTracking', False)
            has_anonymize = telemetry.get('anonymizeData', False)
            has_endpoint = 'exportEndpoint' in telemetry

            score = 0
            if has_enabled:
                score += 20
            if has_metrics:
                score += 20
            if has_tracking:
                score += 20
            if has_anonymize:
                score += 20
            if has_endpoint:
                score += 20

            self._record_test(test_name, score >= 80, f"{score}% configurado", score)
            print(f"  ‚úÖ Sistema de telemetr√≠a: {score}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_dte_knowledge(self):
        """Test 8.1: Validar conocimiento DTE/SII"""
        test_name = "dte_knowledge"
        try:
            kb_file = self.project_root / ".github/agents/knowledge/sii_regulatory_context.md"

            if not kb_file.exists():
                self._record_test(test_name, False, "KB no existe", 0)
                print(f"  ‚ùå Knowledge base SII no encontrada")
                return

            with open(kb_file, 'r') as f:
                content = f.read()

            # Verificar conocimiento cr√≠tico DTE
            dte_knowledge = [
                ('33', 5),   # Factura electr√≥nica
                ('34', 5),   # Factura exenta
                ('52', 5),   # Gu√≠a de despacho
                ('56', 5),   # Nota de d√©bito
                ('61', 5),   # Nota de cr√©dito
                ('Resolution 80/2014', 10),
                ('RUT validation', 10),
                ('modulo 11', 10),
                ('XMLDSig', 10),
                ('CAF', 10),
                ('SII webservice', 10),
                ('folio', 10),
                ('EERGYGROUP', 5)
            ]

            score = 0
            for term, points in dte_knowledge:
                if term in content:
                    score += points

            self._record_test(test_name, score >= 70, f"{score}% conocimiento", score)
            print(f"  ‚úÖ Conocimiento DTE/SII: {score}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_payroll_knowledge(self):
        """Test 8.2: Validar conocimiento de n√≥mina chilena"""
        test_name = "payroll_knowledge"
        try:
            # Buscar conocimiento de n√≥mina en m√∫ltiples fuentes
            sources = [
                self.project_root / ".github/agents/knowledge/project_architecture.md",
                self.project_root / ".github/agents/payroll-compliance.agent.md",
                self.project_root / "AGENTS.md"
            ]

            payroll_terms = [
                ('AFP', 10),
                ('ISAPRE', 10),
                ('APV', 5),
                ('UF', 5),
                ('UTM', 5),
                ('Previred', 10),
                ('total imponible', 10),
                ('90.3 UF', 10),
                ('10%', 5),   # AFP percentage
                ('7%', 5),    # ISAPRE percentage
                ('Labor Code', 10),
                ('economic indicators', 10),
                ('Chilean payroll', 5)
            ]

            total_score = 0
            sources_found = 0

            for source in sources:
                if source.exists():
                    sources_found += 1
                    with open(source, 'r') as f:
                        content = f.read()

                    for term, points in payroll_terms:
                        if term in content:
                            total_score += points / len(sources)  # Dividir puntos entre fuentes

            score = min(100, total_score)

            self._record_test(test_name, score >= 60,
                            f"{sources_found} fuentes, {score:.0f}% conocimiento",
                            score)

            print(f"  ‚úÖ Conocimiento de n√≥mina chilena: {score:.0f}%")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def test_regulatory_framework(self):
        """Test 8.3: Validar marco regulatorio completo"""
        test_name = "regulatory_framework"
        try:
            # Verificar que hay referencias a todas las regulaciones cr√≠ticas
            regulatory_docs = [
                'Resolution 80/2014',      # DTE
                'DL 825',                  # IVA
                'Labor Code',              # N√≥mina
                'Previred',                # Previsi√≥n
                'OWASP Top 10',           # Seguridad
                'ISO 27001',              # Informaci√≥n
                'Chilean Data Protection' # Datos
            ]

            all_files = list(self.project_root.rglob("*.md"))
            regulatory_coverage = {}

            for regulation in regulatory_docs:
                found = False
                for file_path in all_files:
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            if regulation in f.read():
                                found = True
                                break
                    except:
                        continue

                regulatory_coverage[regulation] = found

            found_regulations = sum(1 for v in regulatory_coverage.values() if v)
            score = (found_regulations / len(regulatory_docs)) * 100

            self._record_test(test_name, found_regulations >= len(regulatory_docs) * 0.7,
                            f"{found_regulations}/{len(regulatory_docs)} regulaciones",
                            score)

            print(f"  ‚úÖ Marco regulatorio: {score:.0f}% cobertura")

        except Exception as e:
            self._record_test(test_name, False, str(e), 0)
            print(f"  ‚ùå Error: {e}")

    def _record_test(self, test_name: str, passed: bool, message: str, score: float):
        """Registrar resultado de test"""
        self.results['tests'][test_name] = {
            'passed': passed,
            'message': message,
            'score': score,
            'timestamp': datetime.now().isoformat()
        }
        self.results['scores'][test_name] = score

    def _generate_final_report(self):
        """Generar reporte final de validaci√≥n"""
        print()
        print("=" * 70)
        print("üìä REPORTE FINAL DE VALIDACI√ìN ENTERPRISE")
        print("=" * 70)
        print()

        # Calcular estad√≠sticas
        total_tests = len(self.results['tests'])
        passed_tests = sum(1 for t in self.results['tests'].values() if t['passed'])
        failed_tests = total_tests - passed_tests

        # Score general
        scores = list(self.results['scores'].values())
        avg_score = sum(scores) / len(scores) if scores else 0

        print(f"Total de pruebas ejecutadas: {total_tests}")
        print(f"‚úÖ Pruebas exitosas: {passed_tests}")
        print(f"‚ùå Pruebas fallidas: {failed_tests}")
        print(f"üìä Score promedio: {avg_score:.1f}%")
        print()

        # Categorizar por fase
        phases = {
            'Infraestructura': ['copilot_cli_installation', 'mcp_configuration', 'directory_structure'],
            'Inteligencia': ['agents_intelligence', 'knowledge_base_depth', 'project_understanding'],
            'MCP Servers': ['mcp_servers_configuration', 'custom_mcp_servers', 'mcp_latency'],
            'Memoria': ['persistent_memory', 'session_management', 'cross_project_context'],
            'Seguridad': ['security_policies', 'audit_logging', 'access_control'],
            'CI/CD': ['cicd_workflows', 'automation_pipelines'],
            'M√©tricas': ['metrics_dashboard', 'telemetry_system'],
            'Compliance': ['dte_knowledge', 'payroll_knowledge', 'regulatory_framework']
        }

        print("üìà SCORES POR FASE:")
        print("-" * 70)

        phase_scores = {}
        for phase_name, test_names in phases.items():
            phase_test_scores = [self.results['scores'].get(t, 0) for t in test_names if t in self.results['scores']]
            phase_avg = sum(phase_test_scores) / len(phase_test_scores) if phase_test_scores else 0
            phase_scores[phase_name] = phase_avg

            status_icon = "‚úÖ" if phase_avg >= 80 else "‚ö†Ô∏è " if phase_avg >= 60 else "‚ùå"
            print(f"{status_icon} {phase_name:20s}: {phase_avg:5.1f}%")

        print()

        # Determinar estado general
        if avg_score >= 90:
            overall_status = "üèÜ ENTERPRISE WORLD-CLASS"
            status_color = "üü¢"
        elif avg_score >= 80:
            overall_status = "‚úÖ ENTERPRISE PRODUCTION-READY"
            status_color = "üü¢"
        elif avg_score >= 70:
            overall_status = "üü° ENTERPRISE READY (mejoras recomendadas)"
            status_color = "üü°"
        elif avg_score >= 60:
            overall_status = "üü† FUNCIONAL (requiere mejoras)"
            status_color = "üü†"
        else:
            overall_status = "üî¥ REQUIERE ATENCI√ìN"
            status_color = "üî¥"

        self.results['overall_status'] = overall_status
        self.results['overall_score'] = avg_score

        print("=" * 70)
        print(f"üéØ ESTADO GENERAL: {status_color} {overall_status}")
        print(f"üìä SCORE GLOBAL: {avg_score:.1f}%")
        print("=" * 70)
        print()

        # Guardar reporte JSON en el worktree actual
        report_path = Path("/Users/pedro/.cursor/worktrees/odoo19/usdLt/ENTERPRISE_VALIDATION_REPORT.json")
        with open(report_path, 'w') as f:
            json.dump(self.results, f, indent=2)

        print(f"üìù Reporte completo guardado en: {report_path}")

def main():
    """Funci√≥n principal"""
    suite = EnterpriseValidationSuite()
    results = suite.run_all_tests()

    # Retornar c√≥digo de salida basado en score
    overall_score = results.get('overall_score', 0)
    if overall_score >= 80:
        sys.exit(0)  # √âxito
    else:
        sys.exit(1)  # Mejoras necesarias

if __name__ == "__main__":
    main()

